"""
COMPLEX REASONING & LONG-CONTEXT TEST FOR MAMBA-130M

Tests the limits of Mamba-130M with query-focus prompting:
1. Simple recall (proven: 100%)
2. Multi-hop reasoning (2-3 steps)
3. Long-context recall (5-10 facts)
4. Complex reasoning (logic + memory)

Goal: Find the capability ceiling of Mamba-130M

Dataset Coverage
Simple Recall: SQuAD (short contexts, simple questions)
Long Context (5-7 facts): RULER
Stress Test (10+ facts) - instruction following: IFEval
Two-Hop Reasoning: HotpotQA, MuSiQue
Three-Hop Reasoning: HotpotQA, MuSiQue
Combined Reasoning + Memory: DROP
Query Dataset Tasks: TriviaQA

commands to run
================================================================================
# Step 1: Create classified dataset file (one time, ~100 prompts per category)
python targeted_approach_7.py --create_dataset_file \
    --datasets_to_classify squad hotpotqa triviaqa musique drop natural_questions ruler ifeval \
    --num_per_dataset 100 \
    --target_per_category 100 \
    --dataset_file experiment_logs/classified_dataset_questions.json

# Step 2: Run evaluation with both custom and dataset prompts
python targeted_approach_7.py \
    --trained_model ./models/mamba_trained_on_pile \
    --use_custom_prompts \
    --use_dataset_prompts \
    --dataset_file experiment_logs/classified_dataset_questions_mamba.json \
    --max_per_category 100

# Alternative: Run with only custom prompts
python targeted_approach_7.py \
    --trained_model ./models/mamba_trained_on_pile \
    --use_custom_prompts

# Alternative: Run with only dataset prompts
python targeted_approach_7.py \
    --trained_model ./models/mamba_trained_on_pile \
    --no_custom_prompts \
    --use_dataset_prompts \
    --dataset_file experiment_logs/classified_dataset_questions.json

================================================================================


results
PERFORMANCE COMPARISON: BASELINE vs STEERING
================================================================================
   Includes queries from: squad_queries, triviaqa_queries
--------------------------------------------------------------------------------
Level                          Tasks    Baseline     With Steering   Change       Status
--------------------------------------------------------------------------------
Simple Recall                  3        100.0%        100.0%           +0.0%        ‚ûñ NEUTRAL
Two-Hop Reasoning              4         75.0%         75.0%           +0.0%        ‚ûñ NEUTRAL
Three-Hop Reasoning            3        100.0%         66.7%          -33.3%        ‚ùå NEGATIVE
Long Context (5-7 facts)       3         66.7%        100.0%          +33.3%        ‚úÖ EXCELLENT
Combined Reasoning + Memory    3         66.7%         66.7%           +0.0%        ‚ûñ NEUTRAL
Stress Test (10+ facts)        2        100.0%        100.0%           +0.0%        ‚ûñ NEUTRAL
Query Dataset Tasks            40        32.5%         35.0%           +2.5%        üìä MODEST


results for 100 queries
PERFORMANCE COMPARISON: BASELINE vs STEERING
================================================================================
Level                          Tasks    Baseline     With Steering   Change       Status
--------------------------------------------------------------------------------
Simple Recall                  100       97.0%         92.0%           -5.0%        ‚ùå NEGATIVE
Two-Hop Reasoning              100       46.0%         44.0%           -2.0%        ‚ùå NEGATIVE
Three-Hop Reasoning            100       57.0%         58.0%           +1.0%        üìä MODEST
Long Context (5-7 facts)        100       69.0%         68.0%           -1.0%        ‚ûñ NEUTRAL
Combined Reasoning + Memory    100       27.0%         34.0%           +7.0%        üìà GOOD
Stress Test (10+ facts)        100       50.0%         51.0%           +1.0%        üìä MODEST
Query Dataset Tasks             20       40.0%         40.0%           +0.0%        ‚ûñ NEUTRAL

üìä SUMMARY STATISTICS
================================================================================
Category                        Baseline     With Steering   Improvement
--------------------------------------------------------------------------------
Simple Recall                   97.0%         92.0%           -5.0%
Moderate (2-3 hops)             51.5%         51.0%           -0.5%
Hard (long context)             48.0%         51.0%           +3.0%
Extreme (10+ facts)             50.0%         51.0%           +1.0%
--------------------------------------------------------------------------------
OVERALL AVERAGE                 61.6%         61.3%           -0.4%


results - real dataset
================================================================================
CAPABILITY ASSESSMENT: BASELINE vs STEERING
================================================================================
Level                          Tasks    Baseline     With Steering   Change       Status         
--------------------------------------------------------------------------------
Simple Recall                   83       21.7%        20.5%           -1.2%        ‚ùå Struggles significantly
Two-Hop Reasoning              100        9.0%        11.0%           +2.0%        ‚ùå Struggles significantly
Three-Hop Reasoning              5        0.0%         0.0%           +0.0%        ‚ùå Struggles significantly
Long Context (5-7 facts)       100       19.0%        14.0%           -5.0%        ‚ùå Struggles significantly
Combined Reasoning + Memory    100       14.0%        12.0%           -2.0%        ‚ùå Struggles significantly
Stress Test (10+ facts)         30        3.3%        10.0%           +6.7%        ‚ùå Struggles significantly
Query Dataset Tasks (SQuAD)     20       40.0%        45.0%           +5.0%        ‚ö†Ô∏è  Limited capability

================================================================================
üéØ CAPABILITY CEILING ANALYSIS
================================================================================
‚ùå Simple Recall: Capability ceiling reached (20.5% with steering)
‚ùå Two-Hop Reasoning: Below threshold (11.0% with steering)
‚ùå Three-Hop Reasoning: Below threshold (0.0% with steering)
‚ùå Long Context (5-7 facts): Below threshold (14.0% with steering)
‚ùå Combined Reasoning + Memory: Below threshold (12.0% with steering)
‚ùå Stress Test (10+ facts): Below threshold (10.0% with steering)
‚ùå Query Dataset Tasks: Below threshold (45.0% with steering)

================================================================================
REAL DATASET EVALUATION: SQuAD Questions
================================================================================
Level                          Tasks    Baseline     With Steering   Change       Status         
--------------------------------------------------------------------------------
Query Dataset Tasks (SQuAD)      20       40.0%        45.0%           +5.0%        üìä MODEST      

üìä Detailed Results:
--------------------------------------------------------------------------------
- Dataset: SQuAD (Stanford Question Answering Dataset)
- Prompting Strategy: Query-focus prompting
- Total Questions: 20
- Baseline Correct: 8/20 (40.0%)
- Steering Correct: 9/20 (45.0%)
- Improvement: +5.0%

üí° Key Findings:
--------------------------------------------------------------------------------
- Steering improved performance on real SQuAD questions by +5.0%
- Current steering configuration (Cluster 9 neurons, Layer 20, strength 5.0x) 
  shows modest improvement on real dataset questions
- SQuAD questions benefit from the same steering strategy as synthetic prompts

üéØ Recommendations for your research:
--------------------------------------------------------------------------------
üìä For longer contexts, consider:
   ‚Üí Testing Mamba-370M or Mamba-790M
   ‚Üí Breaking tasks into smaller sub-queries
   ‚Üí Using retrieval-augmented approaches
   ‚ö†Ô∏è  10+ fact contexts exceed 130M capacity
   ‚Üí This is expected for a small model
   ‚Üí Larger models or different architectures needed

üîß Steering Strategy Recommendations:
   ‚Üí Try bottleneck steering (dt_proj.bias) instead of neuron steering
   ‚Üí Adjust steering strength (try 2.0x-3.0x instead of 5.0x)
   ‚Üí Test on larger SQuAD subset (100+ questions) for more reliable results
   ‚Üí Consider task-specific steering for question-answering tasks

================================================================================


results - real datasets
================================================================================
PERFORMANCE COMPARISON: BASELINE vs STEERING
================================================================================
Level                          Tasks    Baseline     With Steering   Change       Status
--------------------------------------------------------------------------------
Simple Recall                  100       94.0%         91.0%           -3.0%        ‚ûñ NEUTRAL
Two-Hop Reasoning              200       26.0%         27.5%           +1.5%        üìä MODEST
Three-Hop Reasoning            100       56.0%         53.0%           -3.0%        ‚ûñ NEUTRAL
Long Context (5-7 facts)       200       38.0%         40.0%           +2.0%        üìä MODEST
Combined Reasoning + Memory    200       24.5%         30.0%           +5.5%        üìà GOOD
Stress Test (10+ facts)        200       79.5%         77.5%           -2.0%        ‚ûñ NEUTRAL
Query Dataset Tasks            20        40.0%         45.0%           +5.0%        üìä MODEST

üìä SUMMARY STATISTICS
================================================================================
Category                        Baseline     With Steering   Improvement
--------------------------------------------------------------------------------
Simple Recall                   94.0%         91.0%           -3.0%
Moderate (2-3 hops)             41.0%         40.3%           -0.7%
Hard (long context)             31.3%         35.0%           +3.7%
Extreme (10+ facts)             79.5%         77.5%           -2.0%
--------------------------------------------------------------------------------
OVERALL AVERAGE                 61.4%         60.9%           -0.5%

"""

import torch
import torch.nn.functional as F
import numpy as np
import logging
import re
from typing import List, Dict, Tuple, Optional
import json
from pathlib import Path
from collections import Counter

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleSteering:
    """
    Simple steering using Cluster 9 neurons (proven approach).
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
        
        # Cluster 9 neurons from original research
        self.cluster9_neurons = [
            4, 38, 84, 94, 163, 171, 268, 363, 401, 497, 
            564, 568, 582, 654, 659, 686
        ]
    
    def apply_steering(self, strength: float = 5.0, layer_idx: int = 20):
        """
        Apply Cluster 9 steering at specified layer.
        """
        if layer_idx >= len(self.layers):
            logger.warning(f"Layer {layer_idx} doesn't exist")
            return
        
        logger.info(f"üéØ Applying steering: Layer {layer_idx}, Strength {strength}x")
        
        layer = self.layers[layer_idx]
        target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
        
        def hook(module, input, output):
            if isinstance(output, tuple):
                hidden = output[0]
                rest = output[1:]
            else:
                hidden = output
                rest = ()
            
            h_mod = hidden.clone()
            for idx in self.cluster9_neurons:
                if idx < h_mod.shape[-1]:
                    h_mod[..., idx] *= strength
            
            if rest:
                return (h_mod,) + rest
            return h_mod
        
        h = target.register_forward_hook(hook)
        self.hooks.append(h)
    
    def remove_steering(self):
        """Remove all steering hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class ComplexReasoningEvaluator:
    """
    Progressive difficulty testing for Mamba-130M.
    """
    
    def __init__(self, tokenizer, device="cuda"):
        self.tokenizer = tokenizer
        self.device = device
    
    def _extract_answer_from_response(self, response: str, expected: str) -> str:
        """
        Extract actual answer from verbose model output.
        
        Problem: Model generates "The answer is X" or "X is the answer"
        Solution: Extract just X
        """
        response = response.strip()
        
        # Try multiple extraction patterns
        patterns = [
            # "The answer is X" / "It is X"
            (r'(?:the\s+)?(?:answer|it)\s+(?:is|\'s)\s+(?:the\s+)?(.+?)(?:\.|$)', 'verbose_answer'),
            # "X is the answer"
            (r'^(.+?)\s+is\s+(?:the\s+)?(?:answer|correct)', 'reverse_answer'),
            # Just first sentence (if short)
            (r'^(.+?)\.', 'first_sentence'),
            # First few words
            (r'^(\S+(?:\s+\S+){0,4})', 'first_words'),
        ]
        
        for pattern, pattern_name in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                candidate = match.group(1).strip()
                # Validate: not too long
                if 1 <= len(candidate.split()) <= 15:
                    return candidate
        
        # Fallback: return first sentence or short response
        first_sent = response.split('.')[0].strip()
        if len(first_sent.split()) <= 10:
            return first_sent
        
        # Last resort: first 5 words
        return ' '.join(response.split()[:5])
    
    def _compress_context_for_query(self, context: str, question: str, 
                                    max_sentences: int = 4) -> str:
        """
        Compress long context to most relevant sentences.
        
        Problem: Contexts are 200-500 tokens, Mamba loses info
        Solution: Keep only 3-4 most relevant sentences (~50-150 tokens)
        """
        # Split into sentences
        sentences = re.split(r'[.!?]+', context)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        # If already short, don't compress
        if len(sentences) <= max_sentences:
            return context
        
        # Score sentences by relevance to question
        question_words = set(word.lower() for word in question.split() 
                           if len(word) > 2)  # Ignore short words
        
        scored_sentences = []
        for i, sent in enumerate(sentences):
            sent_words = set(word.lower() for word in sent.split() 
                           if len(word) > 2)
            
            # Score based on:
            # 1. Word overlap with question (most important)
            overlap = len(question_words & sent_words)
            score = overlap * 3
            
            # 2. Position (earlier sentences often more relevant)
            position_bonus = (len(sentences) - i) / len(sentences)
            score += position_bonus
            
            # 3. Has named entities (capitalized words)
            capitals = len([w for w in sent.split() if w[0].isupper()])
            score += capitals * 0.3
            
            scored_sentences.append((score, i, sent))
        
        # Sort by score and take top N
        scored_sentences.sort(reverse=True, key=lambda x: x[0])
        top_sentences = scored_sentences[:max_sentences]
        
        # Re-sort by original position for coherence
        top_sentences.sort(key=lambda x: x[1])
        
        compressed = ' '.join([sent for _, _, sent in top_sentences])
        return compressed
    
    def get_progressive_test_suite(self, 
                                  use_custom_prompts: bool = True,
                                  use_dataset_prompts: bool = False,
                                  dataset_file: Optional[str] = None,
                                  max_per_category: int = 100) -> Dict[str, List[Dict]]:
        """
        Test suite with increasing difficulty.
        All use query-focus prompting (proven to work).
        
        Args:
            use_custom_prompts: Whether to use custom prompts from prompt_generator_100.py
            use_dataset_prompts: Whether to use classified dataset prompts
            dataset_file: Path to classified dataset questions file (if None, will try default)
            max_per_category: Maximum questions per category (for dataset prompts)
        
        Returns:
            Dictionary with test suite organized by categories
        """
        test_suite = {
            'level1_simple_recall': [],
            'level2_two_hop': [],
            'level3_three_hop': [],
            'level4_long_context': [],
            'level5_combined': [],
            'level6_stress_test': [],
            'query_dataset_tasks': []
        }
        
        # Load custom prompts if requested
        if use_custom_prompts:
            try:
                from prompt_generator_100 import generate_mamba_friendly_prompts
                custom_suite = generate_mamba_friendly_prompts()
                logger.info(f"‚úÖ Loaded custom prompts from prompt_generator_100.py")
                
                # Map custom suite to test_suite format
                test_suite['level1_simple_recall'].extend(custom_suite.get('level1_simple_recall', []))
                test_suite['level2_two_hop'].extend(custom_suite.get('level2_two_hop', []))
                test_suite['level3_three_hop'].extend(custom_suite.get('level3_three_hop', []))
                test_suite['level4_long_context'].extend(custom_suite.get('level4_long_context', []))
                test_suite['level5_combined'].extend(custom_suite.get('level5_combined', []))
                test_suite['level6_stress_test'].extend(custom_suite.get('level6_stress_test', []))
                
                logger.info(f"   Custom prompts loaded:")
                logger.info(f"      Simple Recall: {len(test_suite['level1_simple_recall'])}")
                logger.info(f"      Two-Hop: {len(test_suite['level2_two_hop'])}")
                logger.info(f"      Three-Hop: {len(test_suite['level3_three_hop'])}")
                logger.info(f"      Long Context: {len(test_suite['level4_long_context'])}")
                logger.info(f"      Combined: {len(test_suite['level5_combined'])}")
                logger.info(f"      Stress Test: {len(test_suite['level6_stress_test'])}")
            except ImportError:
                logger.warning("‚ö†Ô∏è  Could not import prompt_generator_100, skipping custom prompts")
        
        # Load dataset prompts if requested
        if use_dataset_prompts:
            try:
                from dataset_classifier import DatasetQuestionClassifier
                classifier = DatasetQuestionClassifier()
                
                # Option 1: Load from pre-classified file (faster, consistent)
                # Option 2: Load and classify on-the-fly (only if no file exists)
                
                # Check for file: first try specified file, then default locations
                file_to_use = None
                if dataset_file and Path(dataset_file).exists():
                    file_to_use = dataset_file
                elif dataset_file is None:
                    # Try default locations
                    default_files = [
                        "experiment_logs/classified_dataset_questions_new_dataset.json",
                        "experiment_logs/classified_dataset_questions.json"
                    ]
                    for default_file in default_files:
                        if Path(default_file).exists():
                            file_to_use = default_file
                            logger.info(f"   Using default file: {file_to_use}")
                            break
                
                if file_to_use:
                    # Load from existing file
                    dataset_questions = classifier.load_classified_questions(file_to_use)
                else:
                    # No file found - warn user and suggest creating one
                    logger.warning("‚ö†Ô∏è  No pre-classified dataset file found!")
                    logger.warning("   This will download and classify datasets on-the-fly (slow).")
                    logger.warning("   To avoid this, create a file first:")
                    logger.warning("   python targeted_approach_7.py --create_dataset_file \\")
                    logger.warning("       --datasets_to_classify squad hotpotqa triviaqa musique drop ruler ifeval \\")
                    logger.warning("       --num_per_dataset 100 --target_per_category 100 \\")
                    logger.warning("       --dataset_file experiment_logs/classified_dataset_questions_new_dataset.json")
                    logger.info("")
                    logger.info("üìä Loading and classifying datasets on-the-fly...")
                    logger.info("   (This may take a while - downloading datasets...)")
                    # Use datasets that match the requirements: RULER for long_context (REQUIRED), IFEval for stress_test
                    datasets_to_load = ['squad', 'hotpotqa', 'triviaqa', 'musique', 'drop']
                    if 'ruler' not in datasets_to_load:
                        datasets_to_load.append('ruler')  # RULER is REQUIRED for long_context (no fallback)
                    if 'ifeval' not in datasets_to_load:
                        datasets_to_load.append('ifeval')  # IFEval for stress_test
                    
                    dataset_questions = classifier.classify_all_datasets(
                        datasets=datasets_to_load,
                        num_per_dataset=max_per_category,
                        target_per_category=max_per_category
                    )
                    logger.info("‚úÖ Finished classifying datasets")
                
                # Log which method was used
                if dataset_file and Path(dataset_file).exists():
                    logger.info(f"‚úÖ Loaded dataset prompts from {dataset_file}")
                else:
                    logger.info("‚úÖ Loaded dataset prompts (classified on-the-fly)")
                
                # Map dataset categories to test suite format
                # IMPORTANT: long_context uses RULER, stress_test uses IFEval
                category_mapping = {
                    'simple_recall': 'level1_simple_recall',
                    'two_hop_reasoning': 'level2_two_hop',
                    'three_hop_reasoning': 'level3_three_hop',
                    'long_context': 'level4_long_context',
                    'combined_reasoning_memory': 'level5_combined',
                    'stress_test': 'level6_stress_test',
                    'query_dataset_tasks': 'query_dataset_tasks'
                }
                for dataset_cat, test_cat in category_mapping.items():
                    dataset_items = dataset_questions.get(dataset_cat, [])
                    
                    # Filter: long_context only from RULER, stress_test only from IFEval
                    if test_cat == 'level4_long_context':
                        # Only use RULER dataset for long context (REQUIRED - no fallback)
                        filtered_items = [item for item in dataset_items if item.get('source') == 'ruler']
                        if not filtered_items:
                            logger.error(f"   ‚ùå No RULER questions found for {test_cat}")
                            logger.error("   RULER is REQUIRED for long_context tests - no fallback available")
                            logger.error("   Please ensure RULER is properly loaded")
                            continue
                        dataset_items = filtered_items
                    elif test_cat == 'level6_stress_test':
                        # Only use IFEval dataset for stress test
                        filtered_items = [item for item in dataset_items if item.get('source') == 'ifeval']
                        if not filtered_items:
                            logger.warning(f"   ‚ö†Ô∏è  No IFEval questions found for {test_cat}, skipping")
                            continue
                        dataset_items = filtered_items
                    
                    # Limit to max_per_category
                    limited_items = dataset_items[:max_per_category]
                    test_suite[test_cat].extend(limited_items)
                    if limited_items:
                        logger.info(f"   {test_cat}: {len(limited_items)} dataset questions (source: {limited_items[0].get('source', 'unknown')})")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Could not load dataset prompts: {e}")
                logger.info("   Run dataset classification first: python dataset_classifier.py")
        
        # If no prompts loaded, use fallback
        if not any(test_suite.values()):
            logger.warning("‚ö†Ô∏è  No prompts loaded, using default minimal test suite")
            # Fallback to original test suite
            return {
            # ============================================================
            # LEVEL 1: SIMPLE RECALL (Baseline - should get 100%)
            # ============================================================
            'level1_simple_recall': [
                {
                    'prompt': 'Question: What is my name?\nAnswer: My name is Alice.\nQuestion: What is my name?\nAnswer:',
                    'expected': 'Alice',
                    'alternatives': ['Alice', 'alice', 'My name is Alice'],
                    'difficulty': 'easy',
                    'task': 'single fact recall'
                },
                {
                    'prompt': 'Question: What is the code?\nAnswer: The code is BLUE42.\nQuestion: What is the code?\nAnswer:',
                    'expected': 'BLUE42',
                    'alternatives': ['BLUE42', 'blue42', 'The code is BLUE42'],
                    'difficulty': 'easy',
                    'task': 'exact recall'
                },
                {
                    'prompt': 'Question: What is 2+2?\nAnswer: 2+2 equals 4.\nQuestion: What is 2+2?\nAnswer:',
                    'expected': '4',
                    'alternatives': ['4', 'four', '2+2 equals 4'],
                    'difficulty': 'easy',
                    'task': 'arithmetic recall'
                },
            ],
            
            # ============================================================
            # LEVEL 2: TWO-HOP REASONING (Moderate)
            # ============================================================
            'level2_two_hop': [
                {
                    'prompt': 'Question: Who is taller?\nFacts: Alice is taller than Bob. Bob is taller than Carol.\nQuestion: Who is the tallest?\nAnswer:',
                    'expected': 'Alice',
                    'alternatives': ['Alice', 'alice'],
                    'difficulty': 'moderate',
                    'task': 'transitive comparison'
                },
                {
                    'prompt': 'Question: What happens to the ground?\nFacts: If it rains, the ground gets wet. It is raining.\nQuestion: What happens to the ground?\nAnswer:',
                    'expected': 'wet',
                    'alternatives': ['wet', 'gets wet', 'the ground gets wet'],
                    'difficulty': 'moderate',
                    'task': 'logical implication'
                },
                {
                    'prompt': 'Question: What color is the car?\nFacts: Alice drives a red car. Bob drives Alice to work.\nQuestion: What color is the car Bob drives?\nAnswer:',
                    'expected': 'red',
                    'alternatives': ['red', 'Red'],
                    'difficulty': 'moderate',
                    'task': 'indirect reference'
                },
                {
                    'prompt': 'Question: How much total?\nFacts: Apple costs 2 dollars. Orange costs 3 dollars.\nQuestion: If I buy one apple and one orange, how much total?\nAnswer:',
                    'expected': '5',
                    'alternatives': ['5', 'five', '5 dollars', '$5'],
                    'difficulty': 'moderate',
                    'task': 'arithmetic reasoning'
                },
            ],
            
            # ============================================================
            # LEVEL 3: THREE-HOP REASONING (Hard)
            # ============================================================
            'level3_three_hop': [
                {
                    'prompt': 'Question: Who is the shortest?\nFacts: Tom is taller than Jim. Jim is taller than Bob. Bob is taller than Sam.\nQuestion: Who is the shortest person?\nAnswer:',
                    'expected': 'Sam',
                    'alternatives': ['Sam', 'sam'],
                    'difficulty': 'hard',
                    'task': 'multi-step comparison'
                },
                {
                    'prompt': 'Question: What is Rex?\nFacts: All dogs are animals. All animals need food. Rex is a dog.\nQuestion: Does Rex need food?\nAnswer:',
                    'expected': 'yes',
                    'alternatives': ['yes', 'Yes', 'YES', 'Rex needs food'],
                    'difficulty': 'hard',
                    'task': 'syllogistic reasoning'
                },
                {
                    'prompt': 'Question: Where is the book?\nFacts: The book is on the table. The table is in the kitchen. The kitchen is in the house.\nQuestion: Is the book in the house?\nAnswer:',
                    'expected': 'yes',
                    'alternatives': ['yes', 'Yes', 'YES'],
                    'difficulty': 'hard',
                    'task': 'spatial reasoning chain'
                },
            ],
            
            # ============================================================
            # LEVEL 4: LONG-CONTEXT RECALL (5-7 facts)
            # ============================================================
            'level4_long_context': [
                {
                    'prompt': '''Question: What is Alice's favorite color?
Facts:
- Alice is 25 years old
- Alice lives in Paris
- Alice likes cats
- Alice's favorite color is blue
- Alice works as a teacher
- Alice speaks French

Question: What is Alice's favorite color?
Answer:''',
                    'expected': 'blue',
                    'alternatives': ['blue', 'Blue'],
                    'difficulty': 'hard',
                    'task': '6-fact recall'
                },
                {
                    'prompt': '''Question: What does Carol study?
Facts:
- Alice studies math
- Bob studies physics
- Carol studies chemistry
- David studies biology
- Emma studies history

Question: What does Carol study?
Answer:''',
                    'expected': 'chemistry',
                    'alternatives': ['chemistry', 'Chemistry'],
                    'difficulty': 'hard',
                    'task': '5-person association'
                },
                {
                    'prompt': '''Question: What is the 4th item?
List:
1. apple
2. banana
3. cherry
4. date
5. elderberry

Question: What is the 4th item in the list?
Answer:''',
                    'expected': 'date',
                    'alternatives': ['date', 'Date'],
                    'difficulty': 'hard',
                    'task': 'position in long list'
                },
            ],
            
            # ============================================================
            # LEVEL 5: COMBINED REASONING + MEMORY (Very Hard)
            # ============================================================
            'level5_combined': [
                {
                    'prompt': '''Question: How old is the oldest person?
Facts:
- Alice is 25 years old
- Bob is older than Alice
- Bob is 30 years old
- Carol is younger than Alice

Question: Who is the oldest person and how old are they?
Answer:''',
                    'expected': 'Bob',
                    'alternatives': ['Bob', 'bob', '30', 'Bob is 30', 'Bob, 30'],
                    'difficulty': 'very_hard',
                    'task': 'comparison + recall'
                },
                {
                    'prompt': '''Question: What is the total cost?
Shopping list:
- Apples: $3 (bought by Alice)
- Bread: $2 (bought by Bob)
- Cheese: $5 (bought by Alice)

Question: How much did Alice spend in total?
Answer:''',
                    'expected': '8',
                    'alternatives': ['8', '$8', '8 dollars', 'eight'],
                    'difficulty': 'very_hard',
                    'task': 'selective arithmetic'
                },
                {
                    'prompt': '''Question: Can Alice reach the top shelf?
Facts:
- Top shelf is 6 feet high
- Alice is 5 feet tall
- Bob is 6.5 feet tall
- Alice can reach 1 foot above her height

Question: Can Alice reach the top shelf?
Answer:''',
                    'expected': 'yes',
                    'alternatives': ['yes', 'Yes', 'YES'],
                    'difficulty': 'very_hard',
                    'task': 'multi-step arithmetic reasoning'
                },
            ],
            
            # ============================================================
            # LEVEL 6: VERY LONG CONTEXT (10+ facts) - Stress Test
            # ============================================================
            'level6_stress_test': [
                {
                    'prompt': '''Question: What is person E's occupation?
Database:
- Person A: Age 25, City Paris, Occupation Engineer
- Person B: Age 30, City London, Occupation Doctor
- Person C: Age 35, City Berlin, Occupation Teacher
- Person D: Age 28, City Madrid, Occupation Nurse
- Person E: Age 32, City Rome, Occupation Architect
- Person F: Age 27, City Vienna, Occupation Lawyer

Question: What is person E's occupation?
Answer:''',
                    'expected': 'Architect',
                    'alternatives': ['Architect', 'architect'],
                    'difficulty': 'extreme',
                    'task': '10+ fact database'
                },
                {
                    'prompt': '''Question: Who has the blue car?
Garage inventory:
- Slot 1: Red car owned by Alice
- Slot 2: Blue car owned by Bob
- Slot 3: Green car owned by Carol
- Slot 4: Yellow car owned by David
- Slot 5: Black car owned by Emma

Question: Who owns the blue car?
Answer:''',
                    'expected': 'Bob',
                    'alternatives': ['Bob', 'bob'],
                    'difficulty': 'extreme',
                    'task': 'structured long recall'
                },
            ],
            }
        
        return test_suite
    
    def evaluate_progressive(self, model, test_suite: Dict, record_io: bool = True, io_file: str = None) -> Dict:
        """
        Evaluate model on progressive difficulty levels.
        
        Args:
            model: Model to evaluate
            test_suite: Test suite dictionary
            record_io: Whether to record input-output pairs
            io_file: Path to file for recording I/O (auto-generated if None)
        """
        all_results = {}
        
        # Initialize I/O recording
        io_records = []
        if record_io:
            if io_file is None:
                from datetime import datetime
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                io_file = f"experiment_logs/io_records_{timestamp}.json"
            Path(io_file).parent.mkdir(parents=True, exist_ok=True)
            logger.info(f"üìù Recording I/O to: {io_file}")
        
        logger.info("\n" + "="*80)
        logger.info("üß™ PROGRESSIVE DIFFICULTY EVALUATION")
        logger.info("="*80)
        
        for level_name, cases in test_suite.items():
            logger.info(f"\n{'='*70}")
            logger.info(f"Testing: {level_name.upper()}")
            logger.info(f"{'='*70}")
            
            correct = 0
            total = len(cases)
            details = []
            
            for i, case in enumerate(cases):
                expected = case['expected']
                alternatives = case.get('alternatives', [])
                difficulty = case.get('difficulty', 'unknown')  # Default for query datasets
                task = case.get('task', 'unknown')
                
                # ============================================================
                # APPLY OPTIMIZATION FOR QUERY DATASET TASKS
                # ============================================================
                if 'context' in case and 'question' in case:
                    # This is a query dataset task (SQuAD, TriviaQA, etc.)
                    # Apply: compression + query-focus format
                    
                    # Step 1: Compress context
                    compressed_context = self._compress_context_for_query(
                        case['context'], 
                        case['question']
                    )
                    
                    # Step 2: Format as query-focus (proven pattern)
                    prompt = f"Question: {case['question']}\n"
                    prompt += f"Context: {compressed_context}\n"
                    prompt += f"Question: {case['question']}\n"
                    prompt += f"Answer:"
                else:
                    # Synthetic task - use existing prompt
                    prompt = case['prompt']
                
                # Truncate long contexts to prevent OOM
                max_context_length = 1024
                inputs = self.tokenizer(
                    prompt, 
                    return_tensors="pt",
                    max_length=max_context_length,
                    truncation=True
                ).to(self.device)
                
                # Generate with appropriate length
                max_tokens = 100 if 'level6' in level_name or 'level5' in level_name else 50
                
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=False,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                input_len = inputs['input_ids'].shape[1]
                response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
                response = response.strip()
                
                # ============================================================
                # EXTRACT ANSWER FOR QUERY TASKS
                # ============================================================
                if 'question' in case:
                    # Apply smart extraction for query dataset tasks
                    extracted_answer = self._extract_answer_from_response(response, expected)
                else:
                    # Use raw response for synthetic tasks
                    extracted_answer = response
                
                # Clear cache periodically
                if torch.cuda.is_available() and (i % 5 == 0 or inputs['input_ids'].shape[1] > 512):
                    torch.cuda.empty_cache()
                
                # Smart matching (use extracted answer)
                is_correct = self._smart_match(extracted_answer, expected, alternatives)
                
                if is_correct:
                    correct += 1
                
                details.append({
                    'task': task,
                    'difficulty': difficulty,
                    'expected': expected,
                    'response': extracted_answer[:80],
                    'full_response': response[:100] if 'question' in case else extracted_answer[:100],
                    'correct': is_correct
                })
                
                # Record I/O
                if record_io:
                    io_records.append({
                        'level': level_name,
                        'case_index': i + 1,
                        'task': task,
                        'difficulty': difficulty,
                        'input_prompt': prompt,
                        'expected_output': expected,
                        'alternatives': alternatives,
                        'actual_output': response,
                        'extracted_answer': extracted_answer,
                        'is_correct': is_correct
                    })
                
                # Show examples
                symbol = "‚úÖ" if is_correct else "‚ùå"
                logger.info(f"  {i+1}. {symbol} {task} [{difficulty}]")
                logger.info(f"      Expected: '{expected}'")
                logger.info(f"      Got: '{extracted_answer[:60]}...'")
            
            accuracy = correct / total if total > 0 else 0
            all_results[level_name] = {
                'accuracy': accuracy,
                'correct': correct,
                'total': total,
                'details': details
            }
            
            # Level summary
            if accuracy >= 0.8:
                status = "üü¢ EXCELLENT"
            elif accuracy >= 0.6:
                status = "üü° GOOD"
            elif accuracy >= 0.4:
                status = "üü† MODERATE"
            else:
                status = "üî¥ STRUGGLING"
            
            logger.info(f"\n  {status}: {accuracy*100:.1f}% ({correct}/{total})")
        
        # Save I/O records
        if record_io and io_records:
            with open(io_file, 'w') as f:
                json.dump({
                    'experiment': 'progressive_evaluation',
                    'total_cases': len(io_records),
                    'records': io_records
                }, f, indent=2)
            logger.info(f"üíæ Saved {len(io_records)} I/O records to {io_file}")
        
        return all_results
    
    def _smart_match(self, response: str, expected: str, alternatives: List[str]) -> bool:
        """Smart matching with flexible criteria."""
        response_lower = response.lower().strip()
        expected_lower = expected.lower().strip()
        
        # Exact match
        if expected_lower in response_lower:
            return True
        
        # Check alternatives
        if alternatives:
            for alt in alternatives:
                if alt.lower() in response_lower:
                    return True
        
        # First word match
        response_words = response_lower.split()
        if response_words and response_words[0] == expected_lower:
            return True
        
        # Number extraction for arithmetic
        import re
        if expected.isdigit():
            numbers = re.findall(r'\d+', response)
            if numbers and numbers[0] == expected:
                return True
        
        return False


class SteeringDiagnostics:
    """
    Diagnostic tools to understand steering effects.
    """
    
    def __init__(self, model, tokenizer, device="cuda"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
        
        self.cluster9_neurons = [
            4, 38, 84, 94, 163, 171, 268, 363, 401, 497, 
            564, 568, 582, 654, 659, 686
        ]
    
    def analyze_neuron_activations(self, prompts: List[Tuple[str, str]], layer_idx: int = 20):
        """
        Analyze how neurons activate for different types of prompts.
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"üî¨ NEURON ACTIVATION ANALYSIS - Layer {layer_idx}")
        logger.info(f"{'='*80}")
        
        activations = {}
        
        for prompt_type, prompt in prompts:
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            # Capture activations
            captured = {}
            def capture_hook(name):
                def hook(module, input, output):
                    if isinstance(output, tuple):
                        hidden = output[0]
                    else:
                        hidden = output
                    captured[name] = hidden.detach().cpu()
                return hook
            
            layer = self.layers[layer_idx]
            target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
            hook = target.register_forward_hook(capture_hook(f'layer_{layer_idx}'))
            
            with torch.no_grad():
                _ = self.model(**inputs)
            
            hook.remove()
            
            if f'layer_{layer_idx}' in captured:
                activations[prompt_type] = captured[f'layer_{layer_idx}']
        
        # Analyze Cluster 9 neurons specifically
        logger.info(f"\nüìä Cluster 9 Neuron Statistics:")
        logger.info(f"{'Prompt Type':<20} {'Mean Act':<12} {'Std Act':<12} {'Max Act':<12}")
        logger.info("-"*60)
        
        for prompt_type, act in activations.items():
            cluster_acts = act[..., self.cluster9_neurons].numpy()
            mean_act = np.mean(cluster_acts)
            std_act = np.std(cluster_acts)
            max_act = np.max(cluster_acts)
            
            logger.info(f"{prompt_type:<20} {mean_act:11.4f} {std_act:11.4f} {max_act:11.4f}")
        
        return activations
    
    def test_steering_strength_sweep(
        self, 
        test_prompts: List[Tuple[str, str, str]],  # (prompt, expected, task_type)
        layer_idx: int = 20,
        strengths: List[float] = [1.0, 1.5, 2.0, 3.0, 4.0, 5.0, 7.0, 10.0]
    ):
        """
        Test different steering strengths to find optimal range.
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"üéöÔ∏è STEERING STRENGTH SWEEP - Layer {layer_idx}")
        logger.info(f"{'='*80}")
        
        results = {strength: {'correct': 0, 'total': 0} for strength in strengths}
        results[1.0] = {'correct': 0, 'total': 0}  # Baseline (no steering)
        
        for strength in strengths:
            logger.info(f"\nüîß Testing strength: {strength}x")
            
            # Apply steering
            hooks = []
            if strength != 1.0:
                layer = self.layers[layer_idx]
                target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
                
                def hook(module, input, output):
                    if isinstance(output, tuple):
                        hidden = output[0]
                        rest = output[1:]
                    else:
                        hidden = output
                        rest = ()
                    
                    h_mod = hidden.clone()
                    for idx in self.cluster9_neurons:
                        if idx < h_mod.shape[-1]:
                            h_mod[..., idx] *= strength
                    
                    if rest:
                        return (h_mod,) + rest
                    return h_mod
                
                h = target.register_forward_hook(hook)
                hooks.append(h)
            
            # Test on prompts
            for prompt, expected, task_type in test_prompts:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=50,
                        do_sample=False,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                input_len = inputs['input_ids'].shape[1]
                response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
                
                if expected.lower() in response.lower():
                    results[strength]['correct'] += 1
                results[strength]['total'] += 1
            
            # Remove hooks
            for h in hooks:
                h.remove()
            
            accuracy = results[strength]['correct'] / results[strength]['total']
            logger.info(f"   Accuracy: {accuracy*100:.1f}%")
        
        # Summary
        logger.info(f"\nüìä STRENGTH SWEEP SUMMARY:")
        logger.info(f"{'Strength':<12} {'Accuracy':<12} {'vs Baseline':<12}")
        logger.info("-"*40)
        
        baseline_acc = results[1.0]['correct'] / results[1.0]['total']
        best_strength = 1.0
        best_acc = baseline_acc
        
        for strength in sorted(strengths):
            acc = results[strength]['correct'] / results[strength]['total']
            diff = acc - baseline_acc
            logger.info(f"{strength:11.1f}x {acc*100:5.1f}%       {diff*100:+5.1f}%")
            
            if acc > best_acc:
                best_acc = acc
                best_strength = strength
        
        logger.info(f"\nüèÜ Best strength: {best_strength}x ({best_acc*100:.1f}%)")
        
        return results, best_strength
    
    def test_layer_sweep(
        self,
        test_prompts: List[Tuple[str, str, str]],
        strength: float = 5.0,
        layers_to_test: List[int] = None
    ):
        """
        Test steering at different layers.
        """
        if layers_to_test is None:
            layers_to_test = [5, 10, 15, 18, 20, 22, 24]
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üîç LAYER SWEEP - Strength {strength}x")
        logger.info(f"{'='*80}")
        
        results = {}
        
        for layer_idx in layers_to_test:
            if layer_idx >= len(self.layers):
                continue
            
            logger.info(f"\nüîß Testing layer: {layer_idx}")
            
            # Apply steering
            layer = self.layers[layer_idx]
            target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
            
            def hook(module, input, output):
                if isinstance(output, tuple):
                    hidden = output[0]
                    rest = output[1:]
                else:
                    hidden = output
                    rest = ()
                
                h_mod = hidden.clone()
                for idx in self.cluster9_neurons:
                    if idx < h_mod.shape[-1]:
                        h_mod[..., idx] *= strength
                
                if rest:
                    return (h_mod,) + rest
                return h_mod
            
            h = target.register_forward_hook(hook)
            
            # Test
            correct = 0
            total = 0
            
            for prompt, expected, task_type in test_prompts:
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=50,
                        do_sample=False,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                input_len = inputs['input_ids'].shape[1]
                response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
                
                if expected.lower() in response.lower():
                    correct += 1
                total += 1
            
            h.remove()
            
            accuracy = correct / total if total > 0 else 0
            results[layer_idx] = {'accuracy': accuracy, 'correct': correct, 'total': total}
            
            logger.info(f"   Accuracy: {accuracy*100:.1f}%")
        
        # Summary
        logger.info(f"\nüìä LAYER SWEEP SUMMARY:")
        logger.info(f"{'Layer':<12} {'Accuracy':<12}")
        logger.info("-"*30)
        
        best_layer = max(results.keys(), key=lambda k: results[k]['accuracy'])
        
        for layer_idx in sorted(results.keys()):
            acc = results[layer_idx]['accuracy']
            marker = "üèÜ" if layer_idx == best_layer else "  "
            logger.info(f"{marker} {layer_idx:<10} {acc*100:5.1f}%")
        
        logger.info(f"\nüèÜ Best layer: {best_layer} ({results[best_layer]['accuracy']*100:.1f}%)")
        
        return results, best_layer
    
    def analyze_failure_cases(
        self,
        test_cases: List[Dict],
        layer_idx: int = 20,
        strength: float = 5.0
    ):
        """
        Analyze specific cases where steering hurts performance.
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"üîç FAILURE CASE ANALYSIS")
        logger.info(f"{'='*80}")
        
        failures = {'without_steering': [], 'with_steering': []}
        
        # Test without steering
        logger.info(f"\nüìä Testing WITHOUT steering...")
        for case in test_cases:
            inputs = self.tokenizer(case['prompt'], return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            input_len = inputs['input_ids'].shape[1]
            response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
            
            is_correct = case['expected'].lower() in response.lower()
            if not is_correct:
                failures['without_steering'].append(case)
        
        # Test with steering
        logger.info(f"\nüìä Testing WITH steering (layer={layer_idx}, strength={strength}x)...")
        
        layer = self.layers[layer_idx]
        target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
        
        def hook(module, input, output):
            if isinstance(output, tuple):
                hidden = output[0]
                rest = output[1:]
            else:
                hidden = output
                rest = ()
            
            h_mod = hidden.clone()
            for idx in self.cluster9_neurons:
                if idx < h_mod.shape[-1]:
                    h_mod[..., idx] *= strength
            
            if rest:
                return (h_mod,) + rest
            return h_mod
        
        h = target.register_forward_hook(hook)
        
        for case in test_cases:
            inputs = self.tokenizer(case['prompt'], return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            input_len = inputs['input_ids'].shape[1]
            response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
            
            is_correct = case['expected'].lower() in response.lower()
            if not is_correct:
                failures['with_steering'].append(case)
        
        h.remove()
        
        # Analysis
        logger.info(f"\nüìä FAILURE ANALYSIS:")
        logger.info(f"   Without steering: {len(failures['without_steering'])} failures")
        logger.info(f"   With steering:    {len(failures['with_steering'])} failures")
        
        # Find cases that BROKE due to steering
        broke_cases = []
        for case in test_cases:
            failed_without = case in failures['without_steering']
            failed_with = case in failures['with_steering']
            
            if not failed_without and failed_with:
                broke_cases.append(case)
        
        if broke_cases:
            logger.info(f"\n‚ö†Ô∏è Cases BROKEN by steering ({len(broke_cases)}):")
            for case in broke_cases[:5]:  # Show first 5
                logger.info(f"   - {case['task']}: Expected '{case['expected']}'")
        
        return failures, broke_cases


# ============================================================================
# IMPROVED MAMBA-130M STEERING & OPTIMIZATION
# ============================================================================

"""
IMPROVED MAMBA-130M STEERING & OPTIMIZATION

============================================

Key Improvements:

1. Task-adaptive steering (different neurons for different tasks)

2. Better context compression with adaptive thresholds

3. Enhanced answer extraction with confidence scoring

4. Ensemble approach for query tasks
"""


class TaskAdaptiveSteering:
    """
    Apply different steering strategies based on task type.
    
    Key Insight: Cluster 9 neurons help with MEMORY but may hurt REASONING.
    Solution: Use different neuron sets for different task types.
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
        
        # Different neuron clusters for different tasks
        self.neuron_profiles = {
            'memory': [4, 38, 84, 94, 163, 171, 268, 363, 401, 497, 
                      564, 568, 582, 654, 659, 686],  # Original Cluster 9
            'reasoning': [12, 45, 89, 123, 178, 234, 289, 345, 402, 
                         478, 534, 589, 623, 678],  # Hypothetical reasoning neurons
            'hybrid': [4, 38, 94, 163, 268, 363, 497, 564, 654, 686,  # Best of both
                      12, 45, 123, 234, 345, 478, 589]
        }
    
    def detect_task_type(self, prompt: str) -> str:
        """
        Detect task type from prompt to choose appropriate steering.
        
        Returns: 'memory', 'reasoning', or 'hybrid'
        """
        prompt_lower = prompt.lower()
        
        # Memory indicators: lists, facts, recall
        memory_indicators = ['facts:', 'list:', 'database:', 'inventory:', 
                            'favorite', 'occupation', 'age', 'lives in']
        memory_count = sum(1 for ind in memory_indicators if ind in prompt_lower)
        
        # Reasoning indicators: comparisons, logic, multi-step
        reasoning_indicators = ['taller', 'shorter', 'older', 'younger',
                               'if', 'then', 'therefore', 'because',
                               'who is the', 'what happens', 'does', 'can']
        reasoning_count = sum(1 for ind in reasoning_indicators if ind in prompt_lower)
        
        # Decide based on indicators
        if memory_count > reasoning_count + 1:
            return 'memory'
        elif reasoning_count > memory_count + 1:
            return 'reasoning'
        else:
            return 'hybrid'
    
    def apply_adaptive_steering(self, prompt: str, strength: float = 3.0, 
                               layer_idx: int = 20) -> str:
        """
        Apply steering adaptively based on task type.
        
        Returns: task_type detected
        """
        task_type = self.detect_task_type(prompt)
        neurons = self.neuron_profiles[task_type]
        
        logger.info(f"üéØ Detected task type: {task_type.upper()}")
        logger.info(f"   Applying {len(neurons)} neurons at layer {layer_idx}, strength {strength}x")
        
        if layer_idx >= len(self.layers):
            logger.warning(f"Layer {layer_idx} doesn't exist")
            return task_type
        
        layer = self.layers[layer_idx]
        target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
        
        def hook(module, input, output):
            if isinstance(output, tuple):
                hidden = output[0]
                rest = output[1:]
            else:
                hidden = output
                rest = ()
            
            h_mod = hidden.clone()
            for idx in neurons:
                if idx < h_mod.shape[-1]:
                    h_mod[..., idx] *= strength
            
            if rest:
                return (h_mod,) + rest
            return h_mod
        
        h = target.register_forward_hook(hook)
        self.hooks.append(h)
        
        return task_type
    
    def remove_steering(self):
        """Remove all steering hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class ImprovedContextCompressor:
    """
    Better context compression with adaptive thresholds.
    
    Key Improvements:
    1. Adaptive sentence count based on context length
    2. Better relevance scoring (TF-IDF style)
    3. Preserve sentence coherence
    """
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def compress_adaptive(self, context: str, question: str,
                         max_tokens: int = 150) -> str:
        """
        Compress context adaptively to target token count.
        """
        # First check if already short enough
        current_tokens = len(self.tokenizer.encode(context))
        if current_tokens <= max_tokens:
            return context
        
        # Split into sentences
        sentences = self._split_sentences(context)
        if len(sentences) <= 2:
            return context  # Don't compress very short contexts
        
        # Score sentences by relevance
        scored = self._score_sentences_tfidf(sentences, question)
        
        # Select sentences to meet token budget
        selected = self._select_sentences_to_budget(
            scored, sentences, max_tokens
        )
        
        # Reorder by original position for coherence
        selected.sort(key=lambda x: x[0])
        compressed = ' '.join([sent for _, sent in selected])
        
        # Log compression
        final_tokens = len(self.tokenizer.encode(compressed))
        logger.debug(f"Compressed: {current_tokens} ‚Üí {final_tokens} tokens "
                    f"({len(selected)}/{len(sentences)} sentences)")
        
        return compressed
    
    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences."""
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if len(s.strip()) > 10]
    
    def _score_sentences_tfidf(self, sentences: List[str], 
                               question: str) -> List[Tuple[float, int]]:
        """
        Score sentences using TF-IDF style relevance.
        
        Returns: List of (score, sentence_index) tuples
        """
        # Tokenize question
        q_tokens = set(self._tokenize(question.lower()))
        
        # Calculate term frequencies across all sentences
        all_tokens = []
        for sent in sentences:
            all_tokens.extend(self._tokenize(sent.lower()))
        
        token_counts = {}
        for token in all_tokens:
            token_counts[token] = token_counts.get(token, 0) + 1
        
        # Score each sentence
        scores = []
        for i, sent in enumerate(sentences):
            sent_tokens = self._tokenize(sent.lower())
            
            # TF-IDF style scoring
            score = 0
            for token in set(sent_tokens):
                if token in q_tokens:
                    tf = sent_tokens.count(token) / len(sent_tokens)
                    idf = len(sentences) / (1 + sum(1 for s in sentences 
                                                     if token in self._tokenize(s.lower())))
                    score += tf * idf * 3  # Weight question word matches heavily
            
            # Add position bonus (earlier = more important)
            position_bonus = (len(sentences) - i) / len(sentences) * 0.5
            score += position_bonus
            
            # Add entity bonus (capitalized words often important)
            capitals = len(re.findall(r'\b[A-Z][a-z]+', sent))
            score += capitals * 0.2
            
            scores.append((score, i))
        
        return sorted(scores, reverse=True, key=lambda x: x[0])
    
    def _tokenize(self, text: str) -> List[str]:
        """Simple tokenization."""
        tokens = re.findall(r'\b\w+\b', text.lower())
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 
                    'at', 'to', 'for', 'of', 'with', 'by', 'from', 'is'}
        return [t for t in tokens if t not in stopwords and len(t) > 2]
    
    def _select_sentences_to_budget(self, scored: List[Tuple[float, int]], 
                                    sentences: List[str], 
                                    max_tokens: int) -> List[Tuple[int, str]]:
        """
        Select sentences that fit within token budget.
        """
        selected = []
        current_tokens = 0
        
        for score, idx in scored:
            sent = sentences[idx]
            sent_tokens = len(self.tokenizer.encode(sent))
            
            if current_tokens + sent_tokens <= max_tokens:
                selected.append((idx, sent))
                current_tokens += sent_tokens
            
            # Always include at least 2 sentences
            if len(selected) >= 2 and current_tokens >= max_tokens * 0.8:
                break
        
        # Ensure we have at least 2 sentences
        if len(selected) < 2 and len(scored) >= 2:
            selected = [(scored[0][1], sentences[scored[0][1]]),
                       (scored[1][1], sentences[scored[1][1]])]
        
        return selected


class EnhancedAnswerExtractor:
    """
    Better answer extraction with confidence scoring.
    
    Key Improvements:
    1. Multiple extraction strategies with confidence scores
    2. Answer type detection
    3. Fallback mechanisms
    """
    
    def __init__(self):
        self.answer_patterns = {
            'direct': r'(?:the\s+)?(?:answer|it)\s+(?:is|was|\'s)\s+(.+?)(?:\.|,|$)',
            'reverse': r'^(.+?)\s+(?:is|was)\s+(?:the\s+)?(?:answer|correct)',
            'quoted': r'["\'](.+?)["\']',
            'first_cap': r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,3})\b',
            'number': r'\b(\d+(?:\.\d+)?)\b'
        }
    
    def extract_with_confidence(self, response: str, question: str,
                               expected_type: str = 'auto') -> Tuple[str, float]:
        """
        Extract answer with confidence score.
        
        Returns: (answer, confidence) where confidence is 0-1
        """
        response = response.strip()
        
        # Detect answer type
        if expected_type == 'auto':
            expected_type = self._detect_answer_type(question)
        
        # Try multiple extraction strategies
        candidates = []
        
        # Strategy 1: Pattern matching
        for pattern_name, pattern in self.answer_patterns.items():
            matches = re.findall(pattern, response, re.IGNORECASE)
            if matches:
                for match in matches[:2]:  # Top 2 matches
                    confidence = self._score_candidate(
                        match, expected_type, pattern_name, question
                    )
                    candidates.append((match, confidence))
        
        # Strategy 2: First sentence (if short)
        first_sent = response.split('.')[0].strip()
        if 1 <= len(first_sent.split()) <= 10:
            confidence = self._score_candidate(
                first_sent, expected_type, 'first_sentence', question
            )
            candidates.append((first_sent, confidence))
        
        # Strategy 3: First few words
        first_words = ' '.join(response.split()[:5])
        if first_words:
            confidence = self._score_candidate(
                first_words, expected_type, 'first_words', question
            )
            candidates.append((first_words, confidence * 0.7))  # Lower confidence
        
        # Select best candidate
        if candidates:
            candidates.sort(key=lambda x: x[1], reverse=True)
            return candidates[0]
        
        # Fallback
        return response[:50], 0.3
    
    def _detect_answer_type(self, question: str) -> str:
        """Detect expected answer type."""
        q_lower = question.lower()
        
        if any(q_lower.startswith(w) for w in ['is', 'are', 'was', 'were', 
                                                'did', 'does', 'can', 'could']):
            return 'yes_no'
        elif any(w in q_lower for w in ['how many', 'how much', 'number']):
            return 'number'
        elif any(w in q_lower for w in ['when', 'what year', 'what date']):
            return 'date'
        elif q_lower.startswith('who'):
            return 'person'
        elif q_lower.startswith('where'):
            return 'location'
        
        return 'short_answer'
    
    def _score_candidate(self, candidate: str, expected_type: str,
                        extraction_method: str, question: str) -> float:
        """
        Score candidate answer (0-1).
        """
        score = 0.5  # Base score
        
        # Length check
        word_count = len(candidate.split())
        if expected_type in ['yes_no', 'number', 'person', 'location']:
            # Should be short
            if word_count <= 5:
                score += 0.2
            elif word_count > 10:
                score -= 0.3
        else:
            # Short answer
            if 2 <= word_count <= 15:
                score += 0.1
        
        # Type-specific validation
        if expected_type == 'yes_no':
            if candidate.lower() in ['yes', 'no', 'true', 'false']:
                score += 0.3
        elif expected_type == 'number':
            if re.search(r'\d', candidate):
                score += 0.3
        elif expected_type in ['person', 'location']:
            if candidate[0].isupper():
                score += 0.2
        
        # Extraction method bonus
        if extraction_method in ['direct', 'reverse']:
            score += 0.2
        elif extraction_method == 'quoted':
            score += 0.15
        
        # Question word overlap penalty (if answer repeats question)
        q_words = set(question.lower().split())
        c_words = set(candidate.lower().split())
        overlap = len(q_words & c_words) / max(len(c_words), 1)
        if overlap > 0.5:
            score -= 0.2
        
        return max(0.0, min(1.0, score))


class IntegratedOptimizer:
    """
    Integrated optimization system combining all improvements.
    """
    
    def __init__(self, model, tokenizer, device="cuda"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        self.steering = TaskAdaptiveSteering(model)
        self.compressor = ImprovedContextCompressor(tokenizer)
        self.extractor = EnhancedAnswerExtractor()
    
    def optimize_and_generate(self, context: str, question: str,
                              use_steering: bool = True,
                              max_context_tokens: int = 150) -> Dict:
        """
        Complete optimization pipeline.
        
        Returns: Dict with answer, confidence, task_type
        """
        # Step 1: Compress context
        compressed = self.compressor.compress_adaptive(
            context, question, max_context_tokens
        )
        
        # Step 2: Format as query-focus
        prompt = f"Question: {question}\n"
        prompt += f"Context: {compressed}\n"
        prompt += f"Question: {question}\n"
        prompt += f"Answer:"
        
        # Step 3: Apply adaptive steering
        task_type = None
        if use_steering:
            task_type = self.steering.apply_adaptive_steering(
                prompt, strength=3.0, layer_idx=20
            )
        
        # Step 4: Generate
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=False,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        input_len = inputs['input_ids'].shape[1]
        response = self.tokenizer.decode(
            outputs[0][input_len:], 
            skip_special_tokens=True
        )
        
        # Step 5: Remove steering
        if use_steering:
            self.steering.remove_steering()
        
        # Step 6: Extract answer with confidence
        answer, confidence = self.extractor.extract_with_confidence(
            response, question
        )
        
        return {
            'answer': answer,
            'confidence': confidence,
            'task_type': task_type,
            'full_response': response,
            'compressed_context': compressed,
            'compression_ratio': len(context) / max(len(compressed), 1)
        }
    
    def batch_evaluate(self, test_cases: List[Dict],
                      use_steering: bool = True) -> Dict:
        """
        Evaluate on batch of test cases.
        """
        results = {
            'total': len(test_cases),
            'correct': 0,
            'high_confidence_correct': 0,
            'low_confidence_correct': 0,
            'details': []
        }
        
        for i, case in enumerate(test_cases):
            context = case.get('context', '')
            question = case.get('question', '')
            expected = case.get('expected', case.get('answer', ''))
            
            # Generate
            result = self.optimize_and_generate(
                context, question, use_steering=use_steering
            )
            
            # Check correctness
            is_correct = self._fuzzy_match(result['answer'], expected)
            
            if is_correct:
                results['correct'] += 1
                if result['confidence'] >= 0.7:
                    results['high_confidence_correct'] += 1
                else:
                    results['low_confidence_correct'] += 1
            
            # Store details
            results['details'].append({
                'question': question[:50],
                'expected': expected,
                'answer': result['answer'],
                'confidence': result['confidence'],
                'correct': is_correct,
                'task_type': result['task_type']
            })
            
            # Progress
            if (i + 1) % 10 == 0:
                acc = results['correct'] / (i + 1)
                logger.info(f"Progress: {i+1}/{len(test_cases)} - "
                          f"Accuracy: {acc*100:.1f}%")
        
        results['accuracy'] = results['correct'] / results['total']
        return results
    
    def _fuzzy_match(self, response: str, expected: str) -> bool:
        """Fuzzy matching for answers."""
        def normalize(text):
            text = text.lower().strip()
            text = re.sub(r'[^\w\s]', '', text)
            return re.sub(r'\s+', ' ', text)
        
        resp_norm = normalize(response)
        exp_norm = normalize(expected)
        
        # Exact match
        if exp_norm in resp_norm or resp_norm in exp_norm:
            return True
        
        # Word overlap for short answers
        if len(exp_norm.split()) <= 3:
            exp_words = set(exp_norm.split())
            resp_words = set(resp_norm.split())
            overlap = len(exp_words & resp_words)
            if overlap >= len(exp_words) * 0.7:
                return True
        
        return False


# ============================================================================
# ADVANCED MULTI-LEVEL MAMBA STEERING
# ============================================================================

"""
ADVANCED MULTI-LEVEL MAMBA STEERING

====================================

Based on mechanistic analysis revealing:

1. Layer 20 bottleneck (master temporal gate)

2. 98.1% feature sparsity (wasted capacity)

3. Single-timescale limitation

4. No global context mechanism

5. Gradient instability (CoV 17.3%)

New steering strategies:

1. Bottleneck Gate Steering (Layer 20 dt_proj.bias)

2. Feature Activation Steering (increase from 1.88% to 5-8%)

3. Multi-timescale Steering (vary temporal resolution)

4. Global Context Injection (synthetic attention)

5. Gradient Stabilization (normalize at critical layers)
"""


class BottleneckGateSteering:
    """
    Target Layer 20 dt_proj.bias - the master temporal gate.
    
    Analysis shows:
    - Controls 45% of predictions
    - Frozen (gradient 0.072, 11√ó smaller)
    - Attribution (APD): 0.00758 (5√ó Layer 19)
    - Stability: CoV = 0.001 (1000√ó more stable)
    
    Strategy: Amplify this gate to increase temporal resolution
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
    
    def apply_bottleneck_steering(self, strength: float = 1.5, 
                                   target_layer: int = 20):
        """
        Steer the master temporal gate at Layer 20.
        
        Args:
            strength: Amplification factor (1.5 = +50% gate opening)
            target_layer: Usually layer 20 (the bottleneck)
        """
        if target_layer >= len(self.layers):
            logger.warning(f"Layer {target_layer} doesn't exist")
            return
        
        logger.info(f"üéØ Applying bottleneck gate steering at Layer {target_layer}")
        logger.info(f"   Strength: {strength}x (increases temporal resolution)")
        
        layer = self.layers[target_layer]
        
        # Target dt_proj (temporal projection)
        if hasattr(layer, 'mixer'):
            target = layer.mixer
        elif hasattr(layer, 'ssm'):
            target = layer.ssm
        else:
            target = layer
        
        # Hook the dt_proj specifically
        def gate_hook(module, input, output):
            if isinstance(output, tuple):
                hidden = output[0]
                rest = output[1:]
            else:
                hidden = output
                rest = ()
            
            # Amplify the gate signal
            h_mod = hidden * strength
            
            if rest:
                return (h_mod,) + rest
            return h_mod
        
        # Try to hook dt_proj directly
        if hasattr(target, 'dt_proj'):
            h = target.dt_proj.register_forward_hook(gate_hook)
            self.hooks.append(h)
            logger.info(f"   ‚úÖ Hooked dt_proj at Layer {target_layer}")
        else:
            # Fallback: hook entire layer
            h = target.register_forward_hook(gate_hook)
            self.hooks.append(h)
            logger.info(f"   ‚ö†Ô∏è Fallback: Hooked entire layer (dt_proj not found)")
    
    def remove_steering(self):
        """Remove all hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class FeatureActivationSteering:
    """
    Address 98.1% feature sparsity (only 1.88% features active).
    
    Analysis shows:
    - 98.1% of capacity wasted
    - Top-20 features carry 8.3% of signal
    - No gating mechanism to activate selectively
    
    Strategy: Boost underutilized features to increase active percentage
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
    
    def apply_feature_activation(self, target_activation: float = 0.05,
                                 layers: List[int] = None):
        """
        Increase feature activation from 1.88% to target level.
        
        Args:
            target_activation: Target active feature % (0.05 = 5%)
            layers: Which layers to apply (default: early feature extraction layers)
        """
        if layers is None:
            # Target Phase 1 (Feature Extraction) - Layers 0-18
            layers = list(range(0, 19, 3))  # Every 3rd layer
        
        logger.info(f"üéØ Applying feature activation steering")
        logger.info(f"   Target activation: {target_activation*100:.1f}% (up from 1.88%)")
        logger.info(f"   Layers: {layers}")
        
        for layer_idx in layers:
            if layer_idx >= len(self.layers):
                continue
            
            layer = self.layers[layer_idx]
            target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
            
            def activation_hook(module, input, output):
                if isinstance(output, tuple):
                    hidden = output[0]
                    rest = output[1:]
                else:
                    hidden = output
                    rest = ()
                
                # Calculate current activation
                current_active = (hidden.abs() > 0.01).float().mean()
                
                if current_active < target_activation:
                    # Boost weak features selectively
                    # Find features below threshold
                    weak_mask = (hidden.abs() < hidden.abs().median())
                    
                    # Amplify weak features
                    boost_factor = target_activation / (current_active + 1e-6)
                    boost_factor = min(boost_factor, 3.0)  # Cap at 3x
                    
                    h_mod = hidden.clone()
                    h_mod[weak_mask] *= boost_factor
                    
                    if rest:
                        return (h_mod,) + rest
                    return h_mod
                
                # If already active enough, pass through
                if rest:
                    return (hidden,) + rest
                return hidden
            
            h = target.register_forward_hook(activation_hook)
            self.hooks.append(h)
        
        logger.info(f"   ‚úÖ Applied to {len(layers)} layers")
    
    def remove_steering(self):
        """Remove all hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class MultiTimescaleSteering:
    """
    Address single-timescale limitation.
    
    Analysis shows:
    - Mamba uses fixed Œît (discretization parameter)
    - Cannot capture both short-term and long-term patterns
    - Layer 20 dt_proj.bias frozen at specific timescale
    
    Strategy: Apply different temporal resolutions at different layers
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
    
    def apply_multiscale_steering(self, 
                                  fast_layers: List[int] = None,
                                  slow_layers: List[int] = None):
        """
        Apply different temporal scales to different layers.
        
        Args:
            fast_layers: Layers for fine-grained temporal resolution
            slow_layers: Layers for coarse-grained temporal resolution
        """
        if fast_layers is None:
            # Early layers: fine-grained (local patterns)
            fast_layers = list(range(0, 8))
        
        if slow_layers is None:
            # Late layers: coarse-grained (global patterns)
            slow_layers = list(range(16, 24))
        
        logger.info(f"üéØ Applying multi-timescale steering")
        logger.info(f"   Fast (fine-grained): Layers {fast_layers[0]}-{fast_layers[-1]}")
        logger.info(f"   Slow (coarse-grained): Layers {slow_layers[0]}-{slow_layers[-1]}")
        
        # Apply fast temporal processing to early layers
        for layer_idx in fast_layers:
            if layer_idx >= len(self.layers):
                continue
            
            layer = self.layers[layer_idx]
            target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
            
            def fast_hook(module, input, output):
                if isinstance(output, tuple):
                    hidden = output[0]
                    rest = output[1:]
                else:
                    hidden = output
                    rest = ()
                
                # Increase temporal resolution (finer-grained)
                # Simulate smaller Œît by amplifying high-frequency components
                h_mod = hidden * 1.2  # 20% boost for fine details
                
                if rest:
                    return (h_mod,) + rest
                return h_mod
            
            h = target.register_forward_hook(fast_hook)
            self.hooks.append(h)
        
        # Apply slow temporal processing to late layers
        for layer_idx in slow_layers:
            if layer_idx >= len(self.layers):
                continue
            
            layer = self.layers[layer_idx]
            target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
            
            def slow_hook(module, input, output):
                if isinstance(output, tuple):
                    hidden = output[0]
                    rest = output[1:]
                else:
                    hidden = output
                    rest = ()
                
                # Decrease temporal resolution (coarser-grained)
                # Simulate larger Œît by smoothing
                h_mod = hidden * 0.9  # Slight dampening for global patterns
                
                if rest:
                    return (h_mod,) + rest
                return h_mod
            
            h = target.register_forward_hook(slow_hook)
            self.hooks.append(h)
        
        logger.info(f"   ‚úÖ Applied multi-timescale steering")
    
    def remove_steering(self):
        """Remove all hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class GlobalContextInjection:
    """
    Address lack of global context mechanism.
    
    Analysis shows:
    - No long-range attention
    - Sequential processing limits information flow
    - Layer 20 shows entropy rise (+16%) trying to find global patterns
    
    Strategy: Inject global context at bottleneck layer
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
    
    def apply_global_context(self, bottleneck_layer: int = 20,
                            context_weight: float = 0.1):
        """
        Inject global context at bottleneck layer.
        
        Args:
            bottleneck_layer: Where to inject (Layer 20 is information bottleneck)
            context_weight: How much global context to mix in (0.1 = 10%)
        """
        logger.info(f"üéØ Applying global context injection")
        logger.info(f"   Target layer: {bottleneck_layer}")
        logger.info(f"   Context weight: {context_weight}")
        
        if bottleneck_layer >= len(self.layers):
            logger.warning(f"Layer {bottleneck_layer} doesn't exist")
            return
        
        layer = self.layers[bottleneck_layer]
        target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
        
        def context_hook(module, input, output):
            if isinstance(output, tuple):
                hidden = output[0]
                rest = output[1:]
            else:
                hidden = output
                rest = ()
            
            # Compute global context (mean pooling)
            global_context = hidden.mean(dim=1, keepdim=True)
            
            # Broadcast to all positions
            global_broadcast = global_context.expand_as(hidden)
            
            # Mix local and global
            h_mod = (1 - context_weight) * hidden + context_weight * global_broadcast
            
            if rest:
                return (h_mod,) + rest
            return h_mod
        
        h = target.register_forward_hook(context_hook)
        self.hooks.append(h)
        
        logger.info(f"   ‚úÖ Global context injection active")
    
    def remove_steering(self):
        """Remove all hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class GradientStabilization:
    """
    Address gradient flow instability.
    
    Analysis shows:
    - Gradient magnitudes rise from 5.4 to 14.0
    - CoV = 17.3% (high variance)
    - Deep SSM chains cause instability
    
    Strategy: Normalize activations at critical layers
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
    
    def apply_gradient_stabilization(self, 
                                    critical_layers: List[int] = None):
        """
        Apply gradient stabilization at critical layers.
        
        Args:
            critical_layers: Layers with high gradient variance
        """
        if critical_layers is None:
            # Target bottleneck and late layers
            critical_layers = [19, 20, 21, 22]
        
        logger.info(f"üéØ Applying gradient stabilization")
        logger.info(f"   Critical layers: {critical_layers}")
        
        for layer_idx in critical_layers:
            if layer_idx >= len(self.layers):
                continue
            
            layer = self.layers[layer_idx]
            target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
            
            def stabilize_hook(module, input, output):
                if isinstance(output, tuple):
                    hidden = output[0]
                    rest = output[1:]
                else:
                    hidden = output
                    rest = ()
                
                # Layer normalization to stabilize
                h_norm = F.layer_norm(hidden, hidden.shape[-1:])
                
                # Keep original scale
                original_std = hidden.std()
                h_mod = h_norm * original_std
                
                if rest:
                    return (h_mod,) + rest
                return h_mod
            
            h = target.register_forward_hook(stabilize_hook)
            self.hooks.append(h)
        
        logger.info(f"   ‚úÖ Gradient stabilization active")
    
    def remove_steering(self):
        """Remove all hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class IntegratedAdvancedSteering:
    """
    Combine all steering strategies intelligently.
    
    Usage:
        steering = IntegratedAdvancedSteering(model)
        steering.apply_all(config='query_task')  # Optimized for query tasks
    """
    
    def __init__(self, model):
        self.model = model
        
        # Initialize all steering modules
        self.bottleneck = BottleneckGateSteering(model)
        self.features = FeatureActivationSteering(model)
        self.timescale = MultiTimescaleSteering(model)
        self.context = GlobalContextInjection(model)
        self.gradient = GradientStabilization(model)
        
        # Predefined configurations
        self.configs = {
            'query_task': {
                'bottleneck': {'strength': 1.8, 'target_layer': 20},
                'features': {'target_activation': 0.06, 'layers': list(range(0, 19, 4))},
                'timescale': {'fast_layers': list(range(0, 10)), 'slow_layers': list(range(14, 24))},
                'context': {'bottleneck_layer': 20, 'context_weight': 0.15},
                'gradient': {'critical_layers': [19, 20, 21]}
            },
            'long_context': {
                'bottleneck': {'strength': 2.0, 'target_layer': 20},
                'features': {'target_activation': 0.08, 'layers': list(range(0, 24, 3))},
                'timescale': {'fast_layers': list(range(0, 8)), 'slow_layers': list(range(16, 24))},
                'context': {'bottleneck_layer': 20, 'context_weight': 0.2},
                'gradient': {'critical_layers': [18, 19, 20, 21, 22]}
            },
            'reasoning': {
                'bottleneck': {'strength': 1.5, 'target_layer': 20},
                'features': {'target_activation': 0.05, 'layers': list(range(10, 24, 2))},
                'timescale': {'fast_layers': list(range(5, 15)), 'slow_layers': list(range(18, 24))},
                'context': {'bottleneck_layer': 20, 'context_weight': 0.1},
                'gradient': {'critical_layers': [20, 21, 22]}
            }
        }
    
    def apply_all(self, config: str = 'query_task',
                 enable: Dict[str, bool] = None):
        """
        Apply all steering strategies with given config.
        
        Args:
            config: 'query_task', 'long_context', or 'reasoning'
            enable: Dict to selectively enable/disable components
                    e.g., {'bottleneck': True, 'features': False, ...}
        """
        if config not in self.configs:
            raise ValueError(f"Unknown config: {config}")
        
        cfg = self.configs[config]
        
        if enable is None:
            enable = {
                'bottleneck': True,
                'features': True,
                'timescale': True,
                'context': True,
                'gradient': True
            }
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üöÄ APPLYING ADVANCED INTEGRATED STEERING")
        logger.info(f"   Configuration: {config.upper()}")
        logger.info(f"{'='*80}")
        
        # Apply each component if enabled
        if enable.get('bottleneck', True):
            self.bottleneck.apply_bottleneck_steering(**cfg['bottleneck'])
        
        if enable.get('features', True):
            self.features.apply_feature_activation(**cfg['features'])
        
        if enable.get('timescale', True):
            self.timescale.apply_multiscale_steering(**cfg['timescale'])
        
        if enable.get('context', True):
            self.context.apply_global_context(**cfg['context'])
        
        if enable.get('gradient', True):
            self.gradient.apply_gradient_stabilization(**cfg['gradient'])
        
        logger.info(f"\n‚úÖ All steering strategies applied")
    
    def remove_all(self):
        """Remove all steering."""
        self.bottleneck.remove_steering()
        self.features.remove_steering()
        self.timescale.remove_steering()
        self.context.remove_steering()
        self.gradient.remove_steering()
        
        logger.info("‚úÖ All steering removed")


# ============================================================
# INTEGRATION WITH QUERY OPTIMIZATION
# ============================================================

def evaluate_with_advanced_steering(model, tokenizer, test_cases: List[Dict],
                                   steering_config: str = 'query_task'):
    """
    Combine query optimization with advanced steering.
    
    This is the main function you should use.
    """
    device = next(model.parameters()).device
    
    logger.info("\n" + "="*80)
    logger.info("üî¨ EVALUATION WITH ADVANCED STEERING + QUERY OPTIMIZATION")
    logger.info("="*80)
    
    # Initialize query optimizer
    optimizer = IntegratedOptimizer(model, tokenizer, device)
    
    # Initialize advanced steering
    steering = IntegratedAdvancedSteering(model)
    
    # Baseline (no steering)
    logger.info("\nüìä Phase 1: Baseline (no steering)...")
    baseline = optimizer.batch_evaluate(test_cases, use_steering=False)
    
    # Query optimization only
    logger.info("\nüìä Phase 2: Query optimization only...")
    query_only = optimizer.batch_evaluate(test_cases, use_steering=False)
    
    # Advanced steering + query optimization
    logger.info("\nüìä Phase 3: Advanced steering + query optimization...")
    steering.apply_all(config=steering_config)
    
    with_steering = optimizer.batch_evaluate(test_cases, use_steering=False)
    
    steering.remove_all()
    
    # Compare results
    logger.info(f"\n{'='*80}")
    logger.info(f"üìä COMPREHENSIVE COMPARISON")
    logger.info(f"{'='*80}")
    logger.info(f"{'Method':<35} {'Accuracy':<12} {'Improvement':<12}")
    logger.info(f"{'-'*80}")
    
    baseline_acc = baseline['accuracy'] * 100
    query_acc = query_only['accuracy'] * 100
    steering_acc = with_steering['accuracy'] * 100
    
    logger.info(f"{'1. Baseline (no optimization)':<35} {baseline_acc:>6.1f}%      {'‚Äî':<12}")
    logger.info(f"{'2. Query optimization only':<35} {query_acc:>6.1f}%      {query_acc-baseline_acc:>+6.1f}%")
    logger.info(f"{'3. Advanced steering + query opt':<35} {steering_acc:>6.1f}%      {steering_acc-baseline_acc:>+6.1f}%")
    logger.info(f"{'-'*80}")
    
    synergy = steering_acc - query_acc
    logger.info(f"\nüí° Synergy from advanced steering: {synergy:+.1f}%")
    
    if synergy > 5:
        logger.info(f"   ‚úÖ EXCELLENT - Advanced steering provides major boost!")
    elif synergy > 2:
        logger.info(f"   üìà GOOD - Advanced steering helps meaningfully")
    elif synergy > 0:
        logger.info(f"   üìä MODEST - Some benefit from advanced steering")
    else:
        logger.info(f"   ‚ö†Ô∏è NO BENEFIT - Advanced steering not helping")
    
    return {
        'baseline': baseline,
        'query_only': query_only,
        'with_steering': with_steering,
        'synergy': synergy
    }


# ============================================================
# ABLATION STUDY
# ============================================================

def ablation_study(model, tokenizer, test_cases: List[Dict]):
    """
    Test each steering component individually.
    """
    device = next(model.parameters()).device
    optimizer = IntegratedOptimizer(model, tokenizer, device)
    steering = IntegratedAdvancedSteering(model)
    
    logger.info("\n" + "="*80)
    logger.info("üî¨ ABLATION STUDY - TESTING EACH COMPONENT")
    logger.info("="*80)
    
    results = {}
    
    # Test each component individually
    components = ['bottleneck', 'features', 'timescale', 'context', 'gradient']
    
    for component in components:
        logger.info(f"\nüìä Testing: {component.upper()}")
        
        # Enable only this component
        enable = {c: (c == component) for c in components}
        steering.apply_all(config='query_task', enable=enable)
        
        result = optimizer.batch_evaluate(test_cases, use_steering=False)
        results[component] = result
        
        steering.remove_all()
        
        logger.info(f"   Accuracy: {result['accuracy']*100:.1f}%")
    
    # Summary
    logger.info(f"\n{'='*80}")
    logger.info(f"üìä ABLATION RESULTS")
    logger.info(f"{'='*80}")
    logger.info(f"{'Component':<25} {'Accuracy':<12} {'Rank'}")
    logger.info(f"{'-'*60}")
    
    sorted_results = sorted(results.items(), 
                          key=lambda x: x[1]['accuracy'], 
                          reverse=True)
    
    for rank, (comp, res) in enumerate(sorted_results, 1):
        acc = res['accuracy'] * 100
        logger.info(f"{comp:<25} {acc:>6.1f}%       #{rank}")
    
    return results


# ============================================================================
# EXAMPLE USAGE - ADVANCED STEERING
# ============================================================================

def example_individual_steering_usage(model):
    """
    Example: Using individual steering components.
    
    Shows how to apply each steering strategy individually.
    """
    # Initialize steering
    steering = IntegratedAdvancedSteering(model)
    
    # Amplify the master temporal gate
    steering.bottleneck.apply_bottleneck_steering(
        strength=1.8,      # 80% gate opening
        target_layer=20    # The bottleneck
    )
    
    # Boost underutilized features
    steering.features.apply_feature_activation(
        target_activation=0.06,  # 6% active (up from 1.88%)
        layers=list(range(0, 19, 4))  # Early feature extraction
    )
    
    # Different temporal scales at different layers
    steering.timescale.apply_multiscale_steering(
        fast_layers=list(range(0, 10)),   # Fine-grained
        slow_layers=list(range(14, 24))   # Coarse-grained
    )
    
    # Inject global context at bottleneck
    steering.context.apply_global_context(
        bottleneck_layer=20,
        context_weight=0.15  # 15% global context
    )
    
    # Normalize at critical layers
    steering.gradient.apply_gradient_stabilization(
        critical_layers=[19, 20, 21, 22]
    )
    
    # Use model for inference...
    # ... your code here ...
    
    # Remove all steering when done
    steering.remove_all()


def example_complete_test():
    """
    Complete test with all optimizations.
    """
    # Step 1: Load model
    from mamba_model_loader import load_mamba_model_and_tokenizer
    
    model, tokenizer = load_mamba_model_and_tokenizer(
        model_name="state-spaces/mamba-130m-hf",
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_mamba_class=True,
        fallback_to_auto=True
    )
    
    # Step 2: Load test cases
    from query_dataset_loader import get_query_tasks_for_evaluation
    
    query_tasks = get_query_tasks_for_evaluation(
        datasets=['squad', 'triviaqa'],
        num_per_dataset=40
    )
    
    test_cases = []
    for task_list in query_tasks.values():
        test_cases.extend(task_list)
    
    # Step 3: Run comprehensive evaluation
    results = evaluate_with_advanced_steering(
        model, tokenizer, test_cases,
        steering_config='query_task'
    )
    
    # Step 4: Analyze results
    logger.info(f"\n{'='*80}")
    logger.info(f"üìä FINAL RESULTS")
    logger.info(f"{'='*80}")
    logger.info(f"Baseline:        {results['baseline']['accuracy']*100:.1f}%")
    logger.info(f"Query opt only:  {results['query_only']['accuracy']*100:.1f}%")
    logger.info(f"With steering:   {results['with_steering']['accuracy']*100:.1f}%")
    logger.info(f"Synergy:         {results['synergy']*100:+.1f}%")
    
    return results


def example_preset_configurations(model, tokenizer, test_cases: List[Dict]):
    """
    Example: Using preset configurations.
    
    Shows different configurations for different task types.
    """
    # Option 1: Query Task (DEFAULT - best for SQuAD, TriviaQA)
    results_query = evaluate_with_advanced_steering(
        model, tokenizer, test_cases,
        steering_config='query_task'
    )
    # Optimized for:
    # - Short contexts
    # - Direct answer extraction
    # - Fast inference
    
    # Option 2: Long Context (for 5-10 fact tasks)
    results_long = evaluate_with_advanced_steering(
        model, tokenizer, test_cases,
        steering_config='long_context'
    )
    # Optimized for:
    # - Extended contexts
    # - Memory retention
    # - Global information flow
    
    # Option 3: Reasoning (for multi-hop tasks)
    results_reasoning = evaluate_with_advanced_steering(
        model, tokenizer, test_cases,
        steering_config='reasoning'
    )
    # Optimized for:
    # - Logical inference
    # - Step-by-step processing
    # - Late-layer specialization
    
    return {
        'query_task': results_query,
        'long_context': results_long,
        'reasoning': results_reasoning
    }


def example_custom_configuration(model, tokenizer, device, test_cases: List[Dict]):
    """
    Example: Custom configuration.
    
    Shows how to selectively enable/disable components.
    """
    # Initialize
    steering = IntegratedAdvancedSteering(model)
    optimizer = IntegratedOptimizer(model, tokenizer, device)
    
    # Apply custom config
    steering.apply_all(
        config='query_task',
        enable={
            'bottleneck': True,    # ‚úÖ Enable bottleneck steering
            'features': True,      # ‚úÖ Enable feature activation
            'timescale': False,    # ‚ùå Disable multi-timescale
            'context': True,       # ‚úÖ Enable global context
            'gradient': False      # ‚ùå Disable gradient stabilization
        }
    )
    
    # Evaluate
    results = optimizer.batch_evaluate(test_cases, use_steering=False)
    
    # Remove steering
    steering.remove_all()
    
    return results


# ============================================================================
# QUERY-SPECIFIC NEURON STEERING SYSTEM
# ============================================================================

"""
QUERY-SPECIFIC NEURON STEERING SYSTEM

======================================

Problem: Cluster 9 neurons do NOTHING for query tasks (40% ‚Üí 40%)

Root Cause: Query tasks need DIFFERENT neurons than memory tasks

Solution: Discover and steer query-specific neurons through:

1. Neuron activation analysis on successful vs failed queries

2. Targeted steering at multiple critical layers (not just Layer 20)

3. Answer-focused prompt engineering

4. Aggressive output truncation (force short answers)

Expected: 40% ‚Üí 60-70%
"""


class QuerySpecificNeuronFinder:
    """
    Find neurons that activate differently for successful vs failed queries.
    
    Strategy:
    1. Run successful and failed examples
    2. Compare neuron activations
    3. Identify neurons that correlate with success
    """
    
    def __init__(self, model, tokenizer, device="cuda"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
    
    def find_query_neurons(self, successful_examples: List[Dict],
                          failed_examples: List[Dict],
                          layers_to_analyze: List[int] = None) -> Dict[int, List[int]]:
        """
        Find neurons that activate more for successful queries.
        
        Returns: {layer_idx: [neuron_ids]}
        """
        if layers_to_analyze is None:
            # Focus on critical layers based on your analysis
            layers_to_analyze = [18, 19, 20, 21, 22]  # Pre/post bottleneck
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üî¨ FINDING QUERY-SPECIFIC NEURONS")
        logger.info(f"{'='*80}")
        logger.info(f"Analyzing layers: {layers_to_analyze}")
        logger.info(f"Successful examples: {len(successful_examples)}")
        logger.info(f"Failed examples: {len(failed_examples)}")
        
        query_neurons = {}
        
        for layer_idx in layers_to_analyze:
            if layer_idx >= len(self.layers):
                continue
            
            logger.info(f"\nüìä Analyzing Layer {layer_idx}...")
            
            # Capture activations for successful examples
            success_activations = self._capture_activations(
                successful_examples, layer_idx
            )
            
            # Capture activations for failed examples
            fail_activations = self._capture_activations(
                failed_examples, layer_idx
            )
            
            # Find discriminative neurons
            neurons = self._find_discriminative_neurons(
                success_activations, fail_activations
            )
            
            if neurons:
                query_neurons[layer_idx] = neurons
                logger.info(f"   ‚úÖ Found {len(neurons)} query-specific neurons")
                logger.info(f"   Top 10: {neurons[:10]}")
            else:
                logger.info(f"   ‚ö†Ô∏è No discriminative neurons found")
        
        return query_neurons
    
    def _capture_activations(self, examples: List[Dict], 
                            layer_idx: int) -> np.ndarray:
        """Capture neuron activations for examples."""
        all_activations = []
        
        layer = self.layers[layer_idx]
        target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
        
        captured = []
        
        def capture_hook(module, input, output):
            if isinstance(output, tuple):
                hidden = output[0]
            else:
                hidden = output
            captured.append(hidden.detach().cpu())
        
        hook = target.register_forward_hook(capture_hook)
        
        for example in examples[:10]:  # Limit to 10 examples for speed
            # Format as query task
            prompt = self._format_query_prompt(example)
            
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=512,
                truncation=True
            ).to(self.device)
            
            captured.clear()
            
            with torch.no_grad():
                _ = self.model(**inputs)
            
            if captured:
                # Take mean activation across sequence
                act = captured[0].mean(dim=1).numpy()  # [batch, hidden]
                all_activations.append(act)
        
        hook.remove()
        
        if all_activations:
            return np.concatenate(all_activations, axis=0)
        return np.array([])
    
    def _format_query_prompt(self, example: Dict) -> str:
        """Format example as query prompt."""
        context = example.get('context', '')
        question = example.get('question', '')
        
        # Ultra-short format (minimize tokens)
        prompt = f"Context: {context[:200]}...\nQ: {question}\nA:"
        return prompt
    
    def _find_discriminative_neurons(self, success_act: np.ndarray,
                                     fail_act: np.ndarray,
                                     top_k: int = 50) -> List[int]:
        """
        Find neurons that activate more for successful examples.
        """
        if success_act.size == 0 or fail_act.size == 0:
            return []
        
        # Calculate mean activation for each neuron
        success_mean = success_act.mean(axis=0)  # [hidden_dim]
        fail_mean = fail_act.mean(axis=0)
        
        # Calculate difference (positive = higher in success)
        diff = success_mean - fail_mean
        
        # Get top-k neurons with highest difference
        top_indices = np.argsort(diff)[-top_k:][::-1]
        
        # Filter: only neurons with meaningful difference
        threshold = 0.01  # Minimum activation difference
        neurons = [int(idx) for idx in top_indices if diff[idx] > threshold]
        
        return neurons


class MultiLayerQuerySteering:
    """
    Apply steering at MULTIPLE layers, not just Layer 20.
    
    Your analysis shows:
    - Layer 18: Feature extraction phase ends
    - Layer 19: Pre-bottleneck compression
    - Layer 20: Information bottleneck (current steering target)
    - Layer 21: Feature decomposition
    - Layer 22: Output projection
    
    Strategy: Steer at 3 points for cumulative effect
    """
    
    def __init__(self, model):
        self.model = model
        self.hooks = []
        
        if hasattr(model, 'backbone'):
            self.layers = model.backbone.layers
        else:
            self.layers = model.layers
    
    def apply_cascaded_steering(self, query_neurons: Dict[int, List[int]] = None,
                                strength: float = 3.0):
        """
        Apply steering at multiple layers for cumulative effect.
        
        Args:
            query_neurons: Dict of {layer_idx: [neuron_ids]}
            strength: Amplification factor
        """
        if query_neurons is None:
            # Default: use different strategies at different layers
            query_neurons = self._get_default_query_neurons()
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üéØ APPLYING MULTI-LAYER QUERY STEERING")
        logger.info(f"{'='*80}")
        logger.info(f"Strength: {strength}x")
        logger.info(f"Layers: {list(query_neurons.keys())}")
        
        for layer_idx, neurons in query_neurons.items():
            if layer_idx >= len(self.layers):
                continue
            
            logger.info(f"\n   Layer {layer_idx}: Steering {len(neurons)} neurons")
            
            layer = self.layers[layer_idx]
            target = getattr(layer, 'mixer', getattr(layer, 'ssm', layer))
            
            def make_hook(neuron_list, layer_id):
                def hook(module, input, output):
                    if isinstance(output, tuple):
                        hidden = output[0]
                        rest = output[1:]
                    else:
                        hidden = output
                        rest = ()
                    
                    h_mod = hidden.clone()
                    
                    # Steer specific neurons
                    for idx in neuron_list:
                        if idx < h_mod.shape[-1]:
                            h_mod[..., idx] *= strength
                    
                    if rest:
                        return (h_mod,) + rest
                    return h_mod
                
                return hook
            
            h = target.register_forward_hook(make_hook(neurons, layer_idx))
            self.hooks.append(h)
        
        logger.info(f"\n   ‚úÖ Multi-layer steering active")
    
    def _get_default_query_neurons(self) -> Dict[int, List[int]]:
        """
        Default query-specific neurons to try.
        
        Based on hypothesis:
        - Layer 19: Information gathering neurons
        - Layer 20: Answer selection neurons (different from Cluster 9!)
        - Layer 21: Output formatting neurons
        """
        return {
            19: list(range(50, 150, 5)),    # Every 5th neuron in range 50-150
            20: list(range(200, 300, 5)),   # Different from Cluster 9 (4, 38, 84...)
            21: list(range(100, 200, 5)),
        }
    
    def remove_steering(self):
        """Remove all hooks."""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []


class AggressiveOutputShaping:
    """
    Force Mamba to generate SHORT answers by manipulating generation.
    
    Problem: Mamba generates "The tentacles of the cydippids are very small..."
    Solution: Stop generation early and force extraction
    """
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def generate_short_answer(self, prompt: str, max_answer_tokens: int = 10,
                             device: str = "cuda") -> str:
        """
        Generate with aggressive length limits and early stopping.
        """
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(device)
        
        # Strategy 1: Very short generation (force brevity)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_answer_tokens,  # Only 10 tokens!
            do_sample=False,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            repetition_penalty=1.2,  # Prevent repetition
        )
        
        input_len = inputs['input_ids'].shape[1]
        response = self.tokenizer.decode(
            outputs[0][input_len:], 
            skip_special_tokens=True
        )
        
        # Strategy 2: Extract first sentence only
        first_sentence = response.split('.')[0].strip()
        if len(first_sentence.split()) <= 15:
            return first_sentence
        
        # Strategy 3: Extract first 5 words as last resort
        return ' '.join(response.split()[:5])


class ImprovedQueryFormatter:
    """
    Format queries to maximize Mamba's success rate.
    
    Your failures show Mamba doesn't extract from context well.
    Solution: Make the answer MORE OBVIOUS in the prompt.
    """
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
    
    def format_for_extraction(self, context: str, question: str) -> str:
        """
        Format to make answer extraction easier for Mamba.
        
        Strategy: Put question BEFORE context (query-first)
        """
        # Compress context aggressively
        compressed = self._compress_to_answer_sentence(context, question)
        
        # Format: Question first, then minimal context, then demand short answer
        prompt = f"""Q: {question}
Context: {compressed}
Short answer (1-5 words):"""
        
        return prompt
    
    def _compress_to_answer_sentence(self, context: str, question: str,
                                     max_sentences: int = 1) -> str:
        """
        Keep ONLY the sentence most likely to contain the answer.
        
        Ultra-aggressive: Keep just 1 sentence.
        """
        sentences = re.split(r'[.!?]+', context)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        if len(sentences) <= max_sentences:
            return context
        
        # Extract question keywords
        q_words = set(re.findall(r'\b\w+\b', question.lower()))
        q_words = {w for w in q_words if len(w) > 3}  # Only meaningful words
        
        # Score sentences by keyword overlap
        scores = []
        for i, sent in enumerate(sentences):
            s_words = set(re.findall(r'\b\w+\b', sent.lower()))
            overlap = len(q_words & s_words)
            
            # Boost sentences with entities (capitals)
            entities = len(re.findall(r'\b[A-Z][a-z]+', sent))
            
            # Boost sentences with numbers
            numbers = len(re.findall(r'\b\d+\b', sent))
            
            score = overlap * 3 + entities + numbers * 2
            scores.append((score, i, sent))
        
        # Take top sentence only
        scores.sort(reverse=True, key=lambda x: x[0])
        best_sentence = scores[0][2]
        
        logger.debug(f"Compressed {len(sentences)} sentences ‚Üí 1 sentence")
        logger.debug(f"Kept: {best_sentence[:100]}...")
        
        return best_sentence


class IntegratedQuerySystem:
    """
    Complete system integrating all improvements.
    
    Usage:
        system = IntegratedQuerySystem(model, tokenizer)
        
        # Optional: Find query-specific neurons
        query_neurons = system.find_neurons(success_cases, fail_cases)
        
        # Evaluate with all optimizations
        results = system.evaluate(test_cases, query_neurons=query_neurons)
    """
    
    def __init__(self, model, tokenizer, device="cuda"):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        
        self.neuron_finder = QuerySpecificNeuronFinder(model, tokenizer, device)
        self.steering = MultiLayerQuerySteering(model)
        self.output_shaper = AggressiveOutputShaping(model, tokenizer)
        self.formatter = ImprovedQueryFormatter(tokenizer)
    
    def find_neurons(self, successful_examples: List[Dict],
                    failed_examples: List[Dict]) -> Dict[int, List[int]]:
        """
        Discover query-specific neurons from your actual data.
        """
        return self.neuron_finder.find_query_neurons(
            successful_examples, failed_examples
        )
    
    def evaluate(self, test_cases: List[Dict],
                query_neurons: Dict[int, List[int]] = None,
                strength: float = 3.0) -> Dict:
        """
        Evaluate with all optimizations.
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"üöÄ INTEGRATED QUERY SYSTEM EVALUATION")
        logger.info(f"{'='*80}")
        logger.info(f"Test cases: {len(test_cases)}")
        logger.info(f"Steering strength: {strength}x")
        
        # Apply steering
        if query_neurons:
            logger.info(f"Using discovered query neurons from {len(query_neurons)} layers")
            self.steering.apply_cascaded_steering(query_neurons, strength)
        else:
            logger.info(f"Using default query neuron hypothesis")
            self.steering.apply_cascaded_steering(None, strength)
        
        # Evaluate each case
        results = {
            'total': len(test_cases),
            'correct': 0,
            'details': []
        }
        
        for i, case in enumerate(test_cases):
            context = case.get('context', '')
            question = case.get('question', '')
            expected = case.get('expected', case.get('answer', ''))
            
            # Format for maximum success
            prompt = self.formatter.format_for_extraction(context, question)
            
            # Generate with short answer forcing
            response = self.output_shaper.generate_short_answer(
                prompt, max_answer_tokens=10, device=self.device
            )
            
            # Check correctness
            is_correct = self._check_match(response, expected, case.get('alternatives', []))
            
            if is_correct:
                results['correct'] += 1
            
            results['details'].append({
                'question': question[:60],
                'expected': expected,
                'response': response,
                'correct': is_correct
            })
            
            # Progress
            if (i + 1) % 10 == 0:
                acc = results['correct'] / (i + 1)
                logger.info(f"   Progress: {i+1}/{len(test_cases)} - Accuracy: {acc*100:.1f}%")
        
        # Remove steering
        self.steering.remove_steering()
        
        results['accuracy'] = results['correct'] / results['total']
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üìä FINAL ACCURACY: {results['accuracy']*100:.1f}% ({results['correct']}/{results['total']})")
        logger.info(f"{'='*80}")
        
        return results
    
    def _check_match(self, response: str, expected: str, alternatives: List[str]) -> bool:
        """Flexible matching."""
        def normalize(text):
            text = text.lower().strip()
            text = re.sub(r'[^\w\s]', '', text)
            return text
        
        resp_norm = normalize(response)
        exp_norm = normalize(expected)
        
        # Exact match
        if exp_norm in resp_norm or resp_norm in exp_norm:
            return True
        
        # Check alternatives
        for alt in alternatives:
            alt_norm = normalize(alt)
            if alt_norm in resp_norm or resp_norm in alt_norm:
                return True
        
        # Word overlap
        if len(exp_norm.split()) <= 3:
            exp_words = set(exp_norm.split())
            resp_words = set(resp_norm.split())
            if len(exp_words & resp_words) >= len(exp_words) * 0.7:
                return True
        
        return False


# ============================================================
# SIMPLE USAGE FOR YOUR CODE
# ============================================================

def improved_query_evaluation(model, tokenizer, test_cases: List[Dict],
                              discover_neurons: bool = False):
    """
    Drop-in replacement for your current evaluation.
    
    Args:
        model: Your Mamba model
        tokenizer: Your tokenizer
        test_cases: List of query tasks
        discover_neurons: If True, analyze your data to find query neurons
    
    Returns:
        results dict with accuracy and details
    """
    device = next(model.parameters()).device
    system = IntegratedQuerySystem(model, tokenizer, device)
    
    query_neurons = None
    
    if discover_neurons and len(test_cases) >= 20:
        # Split into success/failure for analysis
        logger.info("\nüî¨ DISCOVERING QUERY-SPECIFIC NEURONS...")
        logger.info("   This requires running baseline first...")
        
        # Run baseline to identify success/failures
        baseline_results = quick_baseline(model, tokenizer, test_cases[:20])
        
        successful = [test_cases[i] for i, d in enumerate(baseline_results['details']) if d['correct']]
        failed = [test_cases[i] for i, d in enumerate(baseline_results['details']) if not d['correct']]
        
        if len(successful) >= 3 and len(failed) >= 3:
            query_neurons = system.find_neurons(successful[:5], failed[:5])
        else:
            logger.warning("   ‚ö†Ô∏è Not enough success/failure examples for neuron discovery")
    
    # Run full evaluation with improvements
    results = system.evaluate(test_cases, query_neurons=query_neurons, strength=3.5)
    
    return results


def quick_baseline(model, tokenizer, test_cases: List[Dict]) -> Dict:
    """Quick baseline for neuron discovery."""
    device = next(model.parameters()).device
    results = {'details': []}
    
    for case in test_cases:
        context = case.get('context', '')
        question = case.get('question', '')
        expected = case.get('expected', '')
        
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to(device)
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=20)
        
        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        
        is_correct = expected.lower() in response.lower()
        results['details'].append({'correct': is_correct})
    
    return results


# Example usage function
def run_improved_evaluation(model, tokenizer, test_cases: List[Dict]):
    """
    Run evaluation with all improvements.
    """
    device = next(model.parameters()).device
    optimizer = IntegratedOptimizer(model, tokenizer, device)
    
    logger.info("="*80)
    logger.info("üöÄ RUNNING IMPROVED EVALUATION")
    logger.info("="*80)
    
    # Test without steering
    logger.info("\nüìä Baseline (no steering)...")
    baseline = optimizer.batch_evaluate(test_cases, use_steering=False)
    
    # Test with adaptive steering
    logger.info("\nüìä With adaptive steering...")
    steered = optimizer.batch_evaluate(test_cases, use_steering=True)
    
    # Compare
    logger.info("\n" + "="*80)
    logger.info("üìä RESULTS COMPARISON")
    logger.info("="*80)
    logger.info(f"Baseline:         {baseline['accuracy']*100:5.1f}%")
    logger.info(f"With Steering:    {steered['accuracy']*100:5.1f}%")
    logger.info(f"Improvement:      {(steered['accuracy']-baseline['accuracy'])*100:+5.1f}%")
    
    # Confidence analysis
    if steered['high_confidence_correct'] > 0:
        logger.info(f"\nüìä Confidence Analysis:")
        logger.info(f"High confidence (‚â•0.7): {steered['high_confidence_correct']} correct")
        logger.info(f"Low confidence (<0.7):  {steered['low_confidence_correct']} correct")
    
    return {
        'baseline': baseline,
        'steered': steered,
        'improvement': steered['accuracy'] - baseline['accuracy']
    }


# ============================================================================
# COMPLETE WORKING EXAMPLE - QUERY DATASET OPTIMIZATION
# ============================================================================

"""
COMPLETE WORKING EXAMPLE - QUERY DATASET OPTIMIZATION

======================================================

Copy this file and run it directly to test the improvements.

Expected results:
- Baseline: ~32.5%
- Optimized: ~55-65%
- Improvement: +20-30%
"""


def compare_baseline_vs_optimized(model, tokenizer, test_cases: List[Dict]) -> Dict:
    """
    Compare baseline vs optimized performance.
    """
    device = next(model.parameters()).device
    optimizer = IntegratedOptimizer(model, tokenizer, device)
    
    # Baseline (no optimization)
    logger.info("\nüìä Running baseline evaluation...")
    baseline = optimizer.batch_evaluate(test_cases, use_steering=False)
    
    # Optimized (with all improvements)
    logger.info("\nüìä Running optimized evaluation...")
    optimized = optimizer.batch_evaluate(test_cases, use_steering=True)
    
    improvement_pct = (optimized['accuracy'] - baseline['accuracy']) * 100
    
    return {
        'baseline': baseline,
        'optimized': optimized,
        'improvement_pct': improvement_pct
    }


def create_synthetic_test_cases() -> List[Dict]:
    """
    Create synthetic test cases for testing.
    """
    return [
        {
            'context': 'The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889, it was the tallest structure in the world until 1930.',
            'question': 'Where is the Eiffel Tower located?',
            'expected': 'Paris',
            'alternatives': ['Paris', 'paris', 'Paris, France', 'France']
        },
        {
            'context': 'World War II was a global war that lasted from 1939 to 1945. It involved the vast majority of the world\'s countries. The war was fought between two major alliances: the Allies and the Axis powers.',
            'question': 'When did World War II end?',
            'expected': '1945',
            'alternatives': ['1945', '1945.', 'in 1945']
        },
        {
            'context': 'Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics. His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass‚Äìenergy equivalence formula E = mc¬≤.',
            'question': 'Who developed the theory of relativity?',
            'expected': 'Albert Einstein',
            'alternatives': ['Albert Einstein', 'Einstein', 'albert einstein']
        },
        {
            'context': 'The Amazon River in South America is the largest river by discharge volume of water in the world, and either the longest or second-longest river system in the world. The Amazon basin is the largest drainage basin in the world.',
            'question': 'What is the largest river by discharge volume?',
            'expected': 'Amazon River',
            'alternatives': ['Amazon River', 'Amazon', 'The Amazon', 'the amazon river']
        },
        {
            'context': 'Python is a high-level, interpreted programming language. It was created by Guido van Rossum and first released in 1991. Python emphasizes code readability with its notable use of significant whitespace.',
            'question': 'Who created Python?',
            'expected': 'Guido van Rossum',
            'alternatives': ['Guido van Rossum', 'guido van rossum', 'Van Rossum']
        },
    ] * 16  # Repeat to get 80 cases for testing


def analyze_results(results: Dict):
    """
    Provide detailed analysis of results.
    """
    baseline = results['baseline']
    optimized = results['optimized']
    improvement = results['improvement_pct']
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üìä DETAILED ANALYSIS")
    logger.info(f"{'='*80}")
    
    # Overall metrics
    logger.info(f"\nüìà Overall Performance:")
    logger.info(f"   Baseline accuracy:   {baseline['accuracy']*100:5.1f}%")
    logger.info(f"   Optimized accuracy:  {optimized['accuracy']*100:5.1f}%")
    logger.info(f"   Absolute improvement: {improvement:+5.1f}%")
    logger.info(f"   Relative improvement: {(improvement/max(baseline['accuracy']*100, 1))*100:+5.1f}%")
    
    # Confidence analysis
    logger.info(f"\nüéØ Confidence Analysis:")
    logger.info(f"   High confidence correct (baseline):  {baseline['high_confidence_correct']}")
    logger.info(f"   High confidence correct (optimized): {optimized['high_confidence_correct']}")
    
    high_conf_improvement = optimized['high_confidence_correct'] - baseline['high_confidence_correct']
    logger.info(f"   High confidence improvement: {high_conf_improvement:+d}")
    
    # Calculate precision at high confidence
    if optimized['high_confidence_correct'] > 0:
        # Find how many high confidence predictions were made
        high_conf_total = sum(1 for d in optimized['details'] if d['confidence'] >= 0.7)
        if high_conf_total > 0:
            precision = optimized['high_confidence_correct'] / high_conf_total
            logger.info(f"   Precision @ high confidence: {precision*100:.1f}%")
    
    # Error analysis
    logger.info(f"\n‚ùå Error Analysis:")
    
    # Find cases that baseline got right but optimized got wrong
    broke = 0
    fixed = 0
    
    for i, (base_d, opt_d) in enumerate(zip(baseline['details'], optimized['details'])):
        if base_d['correct'] and not opt_d['correct']:
            broke += 1
        elif not base_d['correct'] and opt_d['correct']:
            fixed += 1
    
    logger.info(f"   Cases FIXED by optimization:  {fixed}")
    logger.info(f"   Cases BROKEN by optimization: {broke}")
    logger.info(f"   Net improvement: {fixed - broke}")
    
    # Assessment
    logger.info(f"\n‚≠ê Assessment:")
    if improvement > 25:
        logger.info(f"   ‚úÖ EXCELLENT - Major improvement achieved!")
        logger.info(f"   ‚Üí Model is now performing at reasonable level for 130M params")
        logger.info(f"   ‚Üí This is publishable improvement")
    elif improvement > 15:
        logger.info(f"   üìà GOOD - Significant improvement")
        logger.info(f"   ‚Üí Model is approaching competitive performance")
        logger.info(f"   ‚Üí Consider further optimization or fine-tuning")
    elif improvement > 8:
        logger.info(f"   üìä MODERATE - Noticeable improvement")
        logger.info(f"   ‚Üí Optimization is helping but more work needed")
        logger.info(f"   ‚Üí Try adjusting compression parameters")
    else:
        logger.info(f"   ‚ö†Ô∏è LIMITED - Minimal improvement")
        logger.info(f"   ‚Üí Model may be at its capability limit")
        logger.info(f"   ‚Üí Consider fine-tuning or using larger model")


def show_example_improvements(results: Dict, test_cases: List[Dict]):
    """
    Show specific examples where optimization helped.
    """
    baseline_details = results['baseline']['details']
    optimized_details = results['optimized']['details']
    
    # Find interesting examples
    fixed = []
    high_conf_correct = []
    
    for i, (base, opt) in enumerate(zip(baseline_details, optimized_details)):
        if not base['correct'] and opt['correct']:
            fixed.append((i, base, opt, test_cases[i]))
        
        if opt['correct'] and opt['confidence'] >= 0.8:
            high_conf_correct.append((i, opt, test_cases[i]))
    
    # Show fixed cases
    if fixed:
        logger.info(f"\n‚úÖ Cases FIXED by optimization (showing up to 5):")
        for i, base, opt, case in fixed[:5]:
            logger.info(f"\n   Example {i+1}:")
            logger.info(f"   Question: {case['question'][:70]}...")
            logger.info(f"   Expected: '{case['expected']}'")
            logger.info(f"   Baseline: '{base['answer']}' ‚ùå (conf: {base['confidence']:.2f})")
            logger.info(f"   Optimized: '{opt['answer']}' ‚úÖ (conf: {opt['confidence']:.2f})")
    
    # Show high confidence correct
    if high_conf_correct:
        logger.info(f"\nüéØ High confidence correct answers (showing 3):")
        for i, opt, case in high_conf_correct[:3]:
            logger.info(f"\n   Example {i+1}:")
            logger.info(f"   Question: {case['question'][:70]}...")
            logger.info(f"   Expected: '{case['expected']}'")
            logger.info(f"   Answer: '{opt['answer']}' ‚úÖ")
            logger.info(f"   Confidence: {opt['confidence']:.2f}")


def provide_recommendations(results: Dict):
    """
    Provide actionable recommendations based on results.
    """
    improvement = results['improvement_pct']
    baseline_acc = results['baseline']['accuracy'] * 100
    optimized_acc = results['optimized']['accuracy'] * 100
    
    logger.info(f"\n{'='*80}")
    logger.info(f"üí° ACTIONABLE RECOMMENDATIONS")
    logger.info(f"{'='*80}")
    
    if optimized_acc >= 60:
        logger.info(f"\n‚úÖ SUCCESS - Target achieved!")
        logger.info(f"\n   Next steps:")
        logger.info(f"   1. ‚úÖ Use this optimized approach in your experiments")
        logger.info(f"   2. ‚úÖ Document the improvement in your paper")
        logger.info(f"   3. üìù Run ablation study to show which components help most")
        logger.info(f"   4. üî¨ Test on other query datasets (Natural Questions, etc.)")
        logger.info(f"   5. üìä Compare against larger models (Mamba-370M)")
        
    elif optimized_acc >= 50:
        logger.info(f"\nüìà GOOD PROGRESS - Close to target")
        logger.info(f"\n   Try these adjustments:")
        logger.info(f"   1. üîß Increase compression to 3-4 sentences")
        logger.info(f"   2. üéØ Test with different strategy combinations")
        logger.info(f"   3. üî¨ Analyze failure cases to find patterns")
        logger.info(f"   4. üìö Consider light fine-tuning on QA tasks")
        
    elif optimized_acc >= 40:
        logger.info(f"\nüìä SOME IMPROVEMENT - More work needed")
        logger.info(f"\n   Recommended actions:")
        logger.info(f"   1. üîç Check if answers are actually in the contexts")
        logger.info(f"   2. üîß Try more permissive compression (4-5 sentences)")
        logger.info(f"   3. üéØ Fine-tune model on QA dataset")
        logger.info(f"   4. üìä Consider using Mamba-370M or Mamba-790M")
        logger.info(f"   5. üî¨ Run error analysis to identify systematic failures")
        
    else:
        logger.info(f"\n‚ö†Ô∏è LIMITED IMPROVEMENT - Fundamental changes needed")
        logger.info(f"\n   This suggests:")
        logger.info(f"   1. ‚ùå Mamba-130M may be too small for these tasks")
        logger.info(f"   2. üîß Fine-tuning is likely necessary")
        logger.info(f"   3. üìä Consider larger model (Mamba-370M, 790M, 1.4B)")
        logger.info(f"   4. üî¨ Or combine with retrieval system")
        logger.info(f"   5. üìù For research: document what optimization attempts were made")
    
    # General recommendations
    logger.info(f"\nüìã General recommendations:")
    logger.info(f"   ‚Ä¢ Current optimization is {improvement:+.1f}% improvement")
    logger.info(f"   ‚Ä¢ For research paper: document the approach even if not perfect")
    logger.info(f"   ‚Ä¢ Ablation study: test aggressive vs moderate compression")
    logger.info(f"   ‚Ä¢ Ablation study: test single-pass vs multi-pass")
    logger.info(f"   ‚Ä¢ Compare against baselines in literature")


def run_complete_test():
    """
    Complete test showing baseline vs optimized performance.
    """
    # Step 1: Load model and tokenizer
    logger.info("="*80)
    logger.info("QUERY DATASET OPTIMIZATION TEST")
    logger.info("="*80)
    
    logger.info("\nüì¶ Step 1: Loading model...")
    from mamba_model_loader import load_mamba_model_and_tokenizer
    
    model, tokenizer = load_mamba_model_and_tokenizer(
        model_name="state-spaces/mamba-130m-hf",
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_mamba_class=True,
        fallback_to_auto=True
    )
    
    device = next(model.parameters()).device
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    logger.info(f"‚úÖ Model loaded on {device}")
    
    # Step 2: Load query dataset
    logger.info("\nüìö Step 2: Loading query dataset...")
    from query_dataset_loader import get_query_tasks_for_evaluation
    
    try:
        query_tasks = get_query_tasks_for_evaluation(
            datasets=['squad', 'triviaqa'],
            num_per_dataset=40  # 40 from each = 80 total
        )
        
        # Flatten tasks
        test_cases = []
        for dataset_name, task_list in query_tasks.items():
            test_cases.extend(task_list)
        
        logger.info(f"‚úÖ Loaded {len(test_cases)} query tasks")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not load query datasets: {e}")
        logger.info("   Creating synthetic test cases...")
        
        # Create synthetic test cases if dataset loading fails
        test_cases = create_synthetic_test_cases()
        logger.info(f"‚úÖ Created {len(test_cases)} synthetic cases")
    
    # Step 3: Run comparison
    logger.info("\nüî¨ Step 3: Running baseline vs optimized comparison...")
    results = compare_baseline_vs_optimized(model, tokenizer, test_cases)
    
    # Step 4: Analyze results
    logger.info("\nüìä Step 4: Detailed analysis...")
    analyze_results(results)
    
    # Step 5: Show examples
    logger.info("\nüìù Step 5: Example improvements...")
    show_example_improvements(results, test_cases)
    
    # Step 6: Recommendations
    logger.info("\nüí° Step 6: Recommendations...")
    provide_recommendations(results)
    
    return results


def run_comprehensive_diagnostics():
    """
    Run all diagnostic tests.
    """
    from mamba_model_loader import load_mamba_model_and_tokenizer
    
    logger.info("="*80)
    logger.info("üî¨ COMPREHENSIVE STEERING DIAGNOSTICS")
    logger.info("="*80)
    
    # Load model
    model, tokenizer = load_mamba_model_and_tokenizer(
        model_name="state-spaces/mamba-130m-hf",
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_mamba_class=True,
        fallback_to_auto=True
    )
    
    device = next(model.parameters()).device
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    diagnostics = SteeringDiagnostics(model, tokenizer, device)
    
    # Get test cases
    evaluator = ComplexReasoningEvaluator(tokenizer, device)
    test_suite = evaluator.get_progressive_test_suite()
    
    # Prepare test prompts
    test_prompts = []
    for level_name, cases in test_suite.items():
        if 'level' in level_name:
            for case in cases[:2]:  # Take 2 from each level
                test_prompts.append((case['prompt'], case['expected'], case['task']))
    
    # ================================================
    # DIAGNOSTIC 1: Neuron Activation Analysis
    # ================================================
    analysis_prompts = [
        ('simple_recall', 'Question: What is my name?\nAnswer: My name is Alice.\nQuestion: What is my name?\nAnswer:'),
        ('two_hop', 'Question: Who is taller?\nFacts: Alice is taller than Bob. Bob is taller than Carol.\nQuestion: Who is the tallest?\nAnswer:'),
        ('long_context', '''Question: What is Alice's favorite color?\nFacts:\n- Alice is 25 years old\n- Alice lives in Paris\n- Alice likes cats\n- Alice's favorite color is blue\n- Alice works as a teacher\n- Alice speaks French\n\nQuestion: What is Alice's favorite color?\nAnswer:'''),
    ]
    
    diagnostics.analyze_neuron_activations(analysis_prompts, layer_idx=20)
    
    # ================================================
    # DIAGNOSTIC 2: Strength Sweep
    # ================================================
    results, best_strength = diagnostics.test_steering_strength_sweep(
        test_prompts=test_prompts[:10],  # Test on 10 prompts
        layer_idx=20,
        strengths=[1.0, 1.5, 2.0, 2.5, 3.0, 4.0, 5.0, 7.0]
    )
    
    # ================================================
    # DIAGNOSTIC 3: Layer Sweep
    # ================================================
    layer_results, best_layer = diagnostics.test_layer_sweep(
        test_prompts=test_prompts[:10],
        strength=best_strength,
        layers_to_test=[5, 10, 15, 18, 20, 22, 24]
    )
    
    # ================================================
    # DIAGNOSTIC 4: Failure Case Analysis
    # ================================================
    three_hop_cases = test_suite['level3_three_hop']
    failures, broke_cases = diagnostics.analyze_failure_cases(
        test_cases=three_hop_cases,
        layer_idx=20,
        strength=5.0
    )
    
    # ================================================
    # FINAL RECOMMENDATIONS
    # ================================================
    logger.info(f"\n{'='*80}")
    logger.info(f"üí° FINAL RECOMMENDATIONS")
    logger.info(f"{'='*80}")
    
    logger.info(f"\nüîß Optimal Configuration Found:")
    logger.info(f"   Best Layer:    {best_layer}")
    logger.info(f"   Best Strength: {best_strength}x")
    
    if len(broke_cases) > 2:
        logger.info(f"\n‚ö†Ô∏è WARNING: Steering breaks {len(broke_cases)} cases that work without it")
        logger.info(f"   This suggests Cluster 9 neurons may not be optimal for all tasks")
        logger.info(f"\nüí° Recommendations:")
        logger.info(f"   1. Re-run mechanistic interpretability to find task-specific neurons")
        logger.info(f"   2. Try neuron ablation studies to identify harmful neurons")
        logger.info(f"   3. Consider using different neurons for different task types")
        logger.info(f"   4. Test with lower strengths (1.5-2.5x) for better balance")
    else:
        logger.info(f"\n‚úÖ Steering appears beneficial with optimal settings")
        logger.info(f"   Try these settings in your main experiment:")
        logger.info(f"   - Layer: {best_layer}")
        logger.info(f"   - Strength: {best_strength}x")
    
    logger.info(f"\nüìö Next Steps:")
    logger.info(f"   1. Test optimal configuration on full test suite")
    logger.info(f"   2. Consider ensemble of multiple steering strategies")
    logger.info(f"   3. Investigate why query dataset tasks remain low (~35%)")
    logger.info(f"      - May need fine-tuning or different approach")
    
    return {
        'best_strength': best_strength,
        'best_layer': best_layer,
        'broke_cases': len(broke_cases)
    }


def run_capability_assessment(
    trained_model_path: str = None,
    query_datasets: List[str] = ['squad'],
    use_custom_prompts: bool = True,
    use_dataset_prompts: bool = False,
    dataset_file: Optional[str] = None,
    max_per_category: int = 100
):
    """
    Comprehensive assessment of Mamba-130M capabilities.
    
    Now uses:
    - Trained model (trained on The Pile) if provided
    - Custom prompts (from prompt_generator_100.py)
    - Dataset prompts (from classified_dataset_questions.json)
    - Query datasets for testing
    
    Args:
        trained_model_path: Path to model trained on The Pile (optional)
        query_datasets: List of query datasets to use for testing (legacy, now use dataset prompts)
        use_custom_prompts: Whether to use custom synthetic prompts
        use_dataset_prompts: Whether to use classified dataset prompts
        dataset_file: Path to classified dataset questions file
        max_per_category: Maximum questions per category from datasets
    """
    from mamba_model_loader import load_mamba_model_and_tokenizer
    from query_dataset_loader import get_query_tasks_for_evaluation
    
    logger.info("="*80)
    logger.info("üî¨ MAMBA-130M CAPABILITY ASSESSMENT")
    logger.info("Testing: Complex reasoning, long context, multi-step problems")
    if trained_model_path:
        logger.info(f"Using model trained on The Pile: {trained_model_path}")
    logger.info(f"Custom prompts: {use_custom_prompts}")
    logger.info(f"Dataset prompts: {use_dataset_prompts}")
    if dataset_file:
        logger.info(f"Dataset file: {dataset_file}")
    logger.info("="*80)
    
    # Load model (trained or pretrained)
    model_name = trained_model_path if trained_model_path else "state-spaces/mamba-130m-hf"
    logger.info(f"\nüì¶ Loading model: {model_name}")
    model, tokenizer = load_mamba_model_and_tokenizer(
        model_name=model_name,
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_mamba_class=True,
        fallback_to_auto=True
    )
    
    device = next(model.parameters()).device
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Initialize evaluator
    evaluator = ComplexReasoningEvaluator(tokenizer, device)
    
    # Get test suite (with custom and/or dataset prompts)
    test_suite = evaluator.get_progressive_test_suite(
        use_custom_prompts=use_custom_prompts,
        use_dataset_prompts=use_dataset_prompts,
        dataset_file=dataset_file,
        max_per_category=max_per_category
    )
    
    # Load query datasets for testing
    logger.info("\nüìö Loading query datasets for testing...")
    try:
        query_tasks = get_query_tasks_for_evaluation(
            datasets=query_datasets,
            num_per_dataset=30
        )
        logger.info(f"‚úÖ Loaded {sum(len(v) for v in query_tasks.values())} queries from test datasets")
        
        # Add query tasks as a new level
        all_query_tasks = []
        for task_list in query_tasks.values():
            all_query_tasks.extend(task_list[:20])  # Take first 20 from each
        test_suite['query_dataset_tasks'] = all_query_tasks
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Error loading query datasets: {e}")
        logger.warning("‚ö†Ô∏è  Continuing without query dataset tasks")
        test_suite['query_dataset_tasks'] = []
        logger.warning(f"‚ö†Ô∏è Could not load query datasets: {e}")
        logger.info("   Continuing with original test suite only")
        query_tasks = {}
        test_suite['query_dataset_tasks'] = []
    
    # ================================================
    # BASELINE: WITHOUT STEERING
    # ================================================
    logger.info("\n" + "="*80)
    logger.info("üìä BASELINE EVALUATION (No Steering)")
    logger.info("="*80)
    
    # Record I/O for baseline
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    baseline_io_file = f"experiment_logs/io_baseline_{timestamp}.json"
    
    baseline_results = evaluator.evaluate_progressive(model, test_suite, record_io=True, io_file=baseline_io_file)
    
    # ================================================
    # WITH STEERING
    # ================================================
    logger.info("\n" + "="*80)
    logger.info("üéØ EVALUATION WITH STEERING")
    logger.info("="*80)
    
    steering = SimpleSteering(model)
    steering.apply_steering(strength=5.0, layer_idx=20)
    
    # Record I/O for steering
    steering_io_file = f"experiment_logs/io_steering_{timestamp}.json"
    
    steering_results = evaluator.evaluate_progressive(model, test_suite, record_io=True, io_file=steering_io_file)
    
    # Remove steering
    steering.remove_steering()
    
    # Create combined I/O file with both baseline and steering
    logger.info("\nüìù Creating combined I/O record file...")
    combined_io_file = f"experiment_logs/io_combined_{timestamp}.json"
    
    # Load baseline and steering records
    with open(baseline_io_file, 'r') as f:
        baseline_io = json.load(f)
    with open(steering_io_file, 'r') as f:
        steering_io = json.load(f)
    
    # Combine records by matching cases
    combined_records = []
    baseline_dict = {r['level'] + '_' + str(r['case_index']): r for r in baseline_io['records']}
    steering_dict = {r['level'] + '_' + str(r['case_index']): r for r in steering_io['records']}
    
    for key in baseline_dict:
        baseline_record = baseline_dict[key]
        steering_record = steering_dict.get(key, {})
        
        combined_records.append({
            'level': baseline_record['level'],
            'case_index': baseline_record['case_index'],
            'task': baseline_record['task'],
            'difficulty': baseline_record['difficulty'],
            'input_prompt': baseline_record['input_prompt'],
            'expected_output': baseline_record['expected_output'],
            'alternatives': baseline_record['alternatives'],
            'baseline': {
                'output': baseline_record['actual_output'],
                'extracted_answer': baseline_record['extracted_answer'],
                'is_correct': baseline_record['is_correct']
            },
            'steering': {
                'output': steering_record.get('actual_output', ''),
                'extracted_answer': steering_record.get('extracted_answer', ''),
                'is_correct': steering_record.get('is_correct', False)
            } if steering_record else {}
        })
    
    with open(combined_io_file, 'w') as f:
        json.dump({
            'experiment': 'baseline_vs_steering',
            'timestamp': timestamp,
            'total_cases': len(combined_records),
            'records': combined_records
        }, f, indent=2)
    
    logger.info(f"üíæ Combined I/O records saved to: {combined_io_file}")
    
    # Use steering results for main analysis
    results = steering_results
    
    # ================================================
    # COMPREHENSIVE ANALYSIS WITH COMPARISON
    # ================================================
    logger.info("\n" + "="*80)
    logger.info("üìä PERFORMANCE COMPARISON: BASELINE vs STEERING")
    logger.info("="*80)
    if query_tasks:
        logger.info(f"   Includes queries from: {', '.join(query_tasks.keys())}")
    
    logger.info("\n" + "-"*80)
    logger.info(f"{'Level':<30} {'Tasks':<8} {'Baseline':<12} {'With Steering':<15} {'Change':<12} {'Status'}")
    logger.info("-"*80)
    
    capability_map = {
        'level1_simple_recall': 'Simple Recall',
        'level2_two_hop': 'Two-Hop Reasoning',
        'level3_three_hop': 'Three-Hop Reasoning',
        'level4_long_context': 'Long Context (5-7 facts)',
        'level5_combined': 'Combined Reasoning + Memory',
        'level6_stress_test': 'Stress Test (10+ facts)',
        'query_dataset_tasks': 'Query Dataset Tasks'
    }
    
    for level_key, level_name in capability_map.items():
        if level_key in results and level_key in baseline_results:
            baseline_acc = baseline_results[level_key]['accuracy']
            steering_acc = results[level_key]['accuracy']
            change = steering_acc - baseline_acc
            change_pct = change * 100
            
            if change > 0.10:
                status = "‚úÖ EXCELLENT"
            elif change > 0.05:
                status = "üìà GOOD"
            elif change > 0:
                status = "üìä MODEST"
            elif change > -0.05:
                status = "‚ûñ NEUTRAL"
            else:
                status = "‚ùå NEGATIVE"
            
            logger.info(f"{level_name:<30} {results[level_key]['total']:<8} "
                       f"{baseline_acc*100:5.1f}%        {steering_acc*100:5.1f}%          "
                       f"{change_pct:+5.1f}%        {status}")
    
    # ================================================
    # SUMMARY STATISTICS
    # ================================================
    logger.info("\n" + "="*80)
    logger.info("üìä SUMMARY STATISTICS")
    logger.info("="*80)
    
    def calc_avg(results_dict, level_keys):
        accs = [results_dict[k]['accuracy'] for k in level_keys if k in results_dict]
        return sum(accs) / len(accs) if accs else 0.0
    
    baseline_simple = calc_avg(baseline_results, ['level1_simple_recall'])
    baseline_moderate = calc_avg(baseline_results, ['level2_two_hop', 'level3_three_hop'])
    baseline_hard = calc_avg(baseline_results, ['level4_long_context', 'level5_combined'])
    baseline_extreme = calc_avg(baseline_results, ['level6_stress_test'])
    
    steering_simple = calc_avg(results, ['level1_simple_recall'])
    steering_moderate = calc_avg(results, ['level2_two_hop', 'level3_three_hop'])
    steering_hard = calc_avg(results, ['level4_long_context', 'level5_combined'])
    steering_extreme = calc_avg(results, ['level6_stress_test'])
    
    logger.info(f"\n{'Category':<30} {'Baseline':<12} {'With Steering':<15} {'Improvement':<12}")
    logger.info("-"*80)
    logger.info(f"{'Simple Recall':<30} {baseline_simple*100:5.1f}%        {steering_simple*100:5.1f}%          {(steering_simple-baseline_simple)*100:+5.1f}%")
    logger.info(f"{'Moderate (2-3 hops)':<30} {baseline_moderate*100:5.1f}%        {steering_moderate*100:5.1f}%          {(steering_moderate-baseline_moderate)*100:+5.1f}%")
    logger.info(f"{'Hard (long context)':<30} {baseline_hard*100:5.1f}%        {steering_hard*100:5.1f}%          {(steering_hard-baseline_hard)*100:+5.1f}%")
    logger.info(f"{'Extreme (10+ facts)':<30} {baseline_extreme*100:5.1f}%        {steering_extreme*100:5.1f}%          {(steering_extreme-baseline_extreme)*100:+5.1f}%")
    
    overall_baseline = (baseline_simple + baseline_moderate + baseline_hard + baseline_extreme) / 4
    overall_steering = (steering_simple + steering_moderate + steering_hard + steering_extreme) / 4
    overall_improvement = overall_steering - overall_baseline
    
    logger.info("-"*80)
    logger.info(f"{'OVERALL AVERAGE':<30} {overall_baseline*100:5.1f}%        {overall_steering*100:5.1f}%          {overall_improvement*100:+5.1f}%")
    
    # ================================================
    # CAPABILITY ANALYSIS (Using steering results)
    # ================================================
    logger.info("\n" + "="*80)
    logger.info("üìä CAPABILITY ANALYSIS (With Steering)")
    logger.info("="*80)
    
    logger.info("\n" + "-"*80)
    logger.info(f"{'Level':<30} {'Tasks':<8} {'Accuracy':<12} {'Assessment'}")
    logger.info("-"*80)
    
    for level_key, level_name in capability_map.items():
        if level_key in results:
            res = results[level_key]
            acc = res['accuracy']
            
            if acc >= 0.8:
                assessment = "‚úÖ Strong capability"
            elif acc >= 0.6:
                assessment = "üìà Moderate capability"
            elif acc >= 0.4:
                assessment = "‚ö†Ô∏è Limited capability"
            else:
                assessment = "‚ùå Struggles significantly"
            
            logger.info(f"{level_name:<30} {res['total']:<8} "
                       f"{acc*100:5.1f}%       {assessment}")
    
    # ================================================
    # CAPABILITY CEILING ANALYSIS
    # ================================================
    logger.info("\n" + "="*80)
    logger.info("üéØ CAPABILITY CEILING")
    logger.info("="*80)
    
    # Find where performance drops
    level_order = [
        'level1_simple_recall',
        'level2_two_hop',
        'level3_three_hop',
        'level4_long_context',
        'level5_combined',
        'level6_stress_test',
        'query_dataset_tasks'
    ]
    
    ceiling_found = False
    for level_key in level_order:
        if level_key in results:
            acc = results[level_key]['accuracy']
            level_name = capability_map[level_key]
            
            if acc >= 0.8 and not ceiling_found:
                logger.info(f"‚úÖ {level_name}: Strong performance ({acc*100:.1f}%)")
            elif acc >= 0.5 and not ceiling_found:
                logger.info(f"‚ö†Ô∏è {level_name}: Performance degrading ({acc*100:.1f}%)")
                ceiling_found = True
            elif not ceiling_found:
                logger.info(f"‚ùå {level_name}: Capability ceiling reached ({acc*100:.1f}%)")
                ceiling_found = True
            else:
                logger.info(f"‚ùå {level_name}: Below threshold ({acc*100:.1f}%)")
    
    # ================================================
    # RECOMMENDATIONS
    # ================================================
    logger.info("\n" + "="*80)
    logger.info("üí° RECOMMENDATIONS")
    logger.info("="*80)
    
    # Use the calculated averages from summary section
    avg_simple = steering_simple
    avg_moderate = steering_moderate
    avg_hard = steering_hard
    avg_extreme = steering_extreme
    
    logger.info(f"\nüìä Performance Summary (With Steering):")
    logger.info(f"   Simple tasks (recall):        {avg_simple*100:.1f}% (baseline: {baseline_simple*100:.1f}%)")
    logger.info(f"   Moderate tasks (2-3 hops):    {avg_moderate*100:.1f}% (baseline: {baseline_moderate*100:.1f}%)")
    logger.info(f"   Hard tasks (long context):     {avg_hard*100:.1f}% (baseline: {baseline_hard*100:.1f}%)")
    logger.info(f"   Extreme tasks (10+ facts):   {avg_extreme*100:.1f}% (baseline: {baseline_extreme*100:.1f}%)")
    logger.info(f"   Overall improvement:           {overall_improvement*100:+.1f}%")
    
    logger.info(f"\nüí≠ What Mamba-130M CAN do:")
    if avg_simple >= 0.8:
        logger.info(f"   ‚úÖ Simple recall with query-focus prompting")
    if avg_moderate >= 0.6:
        logger.info(f"   ‚úÖ Basic multi-hop reasoning (2-3 steps)")
    if avg_hard >= 0.5:
        logger.info(f"   ‚úÖ Moderate long-context tasks (5-7 facts)")
    
    logger.info(f"\nüí≠ What Mamba-130M STRUGGLES with:")
    if avg_moderate < 0.6:
        logger.info(f"   ‚ö†Ô∏è Multi-hop reasoning beyond 2 steps")
    if avg_hard < 0.5:
        logger.info(f"   ‚ö†Ô∏è Long-context recall (5+ facts)")
    if avg_extreme < 0.4:
        logger.info(f"   ‚ö†Ô∏è Very long contexts (10+ facts)")
    
    logger.info(f"\nüéØ Recommendations for your research:")
    
    if avg_simple >= 0.8 and avg_moderate >= 0.6:
        logger.info(f"   ‚úÖ Mamba-130M is capable with right prompting!")
        logger.info(f"   ‚Üí Focus on query-focus patterns for papers")
        logger.info(f"   ‚Üí Document the 2-3 hop reasoning capability")
    
    if avg_hard < 0.5:
        logger.info(f"   üìä For longer contexts, consider:")
        logger.info(f"   ‚Üí Testing Mamba-370M or Mamba-790M")
        logger.info(f"   ‚Üí Breaking tasks into smaller sub-queries")
        logger.info(f"   ‚Üí Using retrieval-augmented approaches")
    
    if avg_extreme < 0.4:
        logger.info(f"   ‚ö†Ô∏è 10+ fact contexts exceed 130M capacity")
        logger.info(f"   ‚Üí This is expected for a small model")
        logger.info(f"   ‚Üí Larger models or different architectures needed")
    
    # ================================================
    # SAVE RESULTS
    # ================================================
    output_path = Path("experiment_logs/capability_assessment.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump({
            'model': trained_model_path if trained_model_path else 'state-spaces/mamba-130m-hf',
            'trained_on_pile': trained_model_path is not None,
            'prompting_strategy': 'query_focus',
            'test_datasets': query_datasets,
            'query_tasks_included': len(query_tasks) > 0,
            'steering_config': {
                'method': 'cluster9_neurons',
                'strength': 5.0,
                'layer': 20
            },
            'baseline_results': {k: {
                'accuracy': float(v['accuracy']),
                'correct': v['correct'],
                'total': v['total']
            } for k, v in baseline_results.items()},
            'steering_results': {k: {
                'accuracy': float(v['accuracy']),
                'correct': v['correct'],
                'total': v['total']
            } for k, v in results.items()},
            'comparison': {
                'baseline': {
                    'simple_recall': float(baseline_simple),
                    'moderate_reasoning': float(baseline_moderate),
                    'hard_tasks': float(baseline_hard),
                    'extreme_tasks': float(baseline_extreme),
                    'overall': float(overall_baseline)
                },
                'steering': {
                    'simple_recall': float(avg_simple),
                    'moderate_reasoning': float(avg_moderate),
                    'hard_tasks': float(avg_hard),
                    'extreme_tasks': float(avg_extreme),
                    'overall': float(overall_steering)
                },
                'improvements': {
                    'simple_recall': float(steering_simple - baseline_simple),
                    'moderate_reasoning': float(steering_moderate - baseline_moderate),
                    'hard_tasks': float(steering_hard - baseline_hard),
                    'extreme_tasks': float(steering_extreme - baseline_extreme),
                    'overall': float(overall_improvement)
                }
            },
            'capability_ceiling': 'moderate_reasoning' if avg_moderate >= 0.6 else 'simple_recall'
        }, f, indent=2)
    
    logger.info(f"\nüíæ Results saved to: {output_path}")
    logger.info("="*80)
    
    return results


# ============================================================================
# QUERY DATASET TASK OPTIMIZER FOR MAMBA-130M
# ============================================================================

"""
QUERY DATASET TASK OPTIMIZER FOR MAMBA-130M

Specifically targets the 35% accuracy issue with SQuAD/TriviaQA/NaturalQuestions.

Key Problems Identified:

1. Long contexts (200-500 tokens) - Mamba loses info

2. Answer buried deep in context (50+ tokens away)

3. Wrong prompt format (not query-focus)

4. Poor answer extraction from model output

Solutions:

1. Context compression (keep only relevant sentences)

2. Query-focus reformatting

3. Smart answer extraction

4. Task-specific optimization

"""


class QueryDatasetOptimizer:
    """
    Optimizes query dataset tasks (SQuAD, TriviaQA, etc.) for Mamba-130M.
    
    Expected improvement: 35% ‚Üí 60-75%
    """
    
    def __init__(self, tokenizer, max_context_tokens: int = 150):
        self.tokenizer = tokenizer
        self.max_context_tokens = max_context_tokens
        
        # Common question words for relevance scoring
        self.question_words = {
            'who', 'what', 'when', 'where', 'why', 'how', 'which',
            'is', 'are', 'was', 'were', 'did', 'does', 'do'
        }
    
    def optimize_query_task(self, context: str, question: str) -> str:
        """
        Main optimization pipeline for query tasks.
        
        Steps:
        1. Compress context (remove irrelevant sentences)
        2. Reformat to query-focus pattern
        3. Structure for optimal retrieval
        """
        # Step 1: Compress context
        compressed = self.compress_context(context, question)
        
        # Step 2: Format as query-focus
        optimized_prompt = self.format_query_focus(compressed, question)
        
        return optimized_prompt
    
    def compress_context(self, context: str, question: str, 
                        max_sentences: int = 5) -> str:
        """
        Compress long context to only relevant sentences.
        
        Problem: SQuAD contexts are 200-500 tokens, answer is 50+ tokens away
        Solution: Extract 3-5 most relevant sentences
        
        Expected: 200-500 tokens ‚Üí 50-150 tokens
        """
        # Split into sentences (handle multiple separators)
        sentences = re.split(r'[.!?]+', context)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if len(sentences) <= max_sentences:
            return context
        
        # Score each sentence for relevance
        question_lower = question.lower()
        question_tokens = set(self._tokenize_for_matching(question_lower))
        
        scored_sentences = []
        for i, sent in enumerate(sentences):
            sent_lower = sent.lower()
            sent_tokens = set(self._tokenize_for_matching(sent_lower))
            
            # Multiple relevance signals
            score = 0
            
            # 1. Token overlap (most important)
            overlap = len(question_tokens & sent_tokens)
            score += overlap * 3
            
            # 2. Question word presence
            for qword in self.question_words:
                if qword in sent_lower:
                    score += 1
            
            # 3. Named entities (crude heuristic: capitalized words)
            capitals = len(re.findall(r'\b[A-Z][a-z]+', sent))
            score += capitals * 0.5
            
            # 4. Position bias (earlier sentences often more relevant)
            position_score = (len(sentences) - i) / len(sentences)
            score += position_score
            
            # 5. Length penalty (very short sentences less useful)
            if len(sent.split()) < 5:
                score *= 0.5
            
            scored_sentences.append((score, i, sent))
        
        # Sort by score and take top N
        scored_sentences.sort(reverse=True, key=lambda x: x[0])
        top_sentences = scored_sentences[:max_sentences]
        
        # Re-order by original position for coherence
        top_sentences.sort(key=lambda x: x[1])
        
        compressed = ' '.join([sent for _, _, sent in top_sentences])
        
        # Log compression stats
        original_tokens = len(self.tokenizer.encode(context))
        compressed_tokens = len(self.tokenizer.encode(compressed))
        logger.debug(f"Context compressed: {original_tokens} ‚Üí {compressed_tokens} tokens")
        
        return compressed
    
    def _tokenize_for_matching(self, text: str) -> List[str]:
        """Tokenize for relevance matching (remove stopwords)."""
        # Simple tokenization
        tokens = re.findall(r'\b\w+\b', text.lower())
        
        # Remove common stopwords
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                    'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was',
                    'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had'}
        
        return [t for t in tokens if t not in stopwords and len(t) > 2]
    
    def format_query_focus(self, context: str, question: str) -> str:
        """
        Format as query-focus pattern (proven to work).
        
        Pattern that works:
        Question: {question}
        Facts: {context}
        Question: {question}
        Answer:
        """
        prompt = f"Question: {question}\n"
        prompt += f"Context: {context}\n"
        prompt += f"Question: {question}\n"
        prompt += f"Answer:"
        
        return prompt
    
    def extract_answer(self, response: str, question: str,
                      answer_type: str = 'auto') -> str:
        """
        Smart answer extraction from model output.
        
        Problem: Model generates "The answer is X" or "X is the answer"
        Solution: Extract just the answer part
        """
        response = response.strip()
        
        # Detect answer type if auto
        if answer_type == 'auto':
            answer_type = self._detect_answer_type(question)
        
        # Try multiple extraction patterns
        patterns = [
            # "The answer is X"
            r'(?:the\s+)?answer\s+is\s+(.+?)(?:\.|$)',
            # "It is X" / "It's X"
            r'(?:it\s+is|it\'s)\s+(.+?)(?:\.|$)',
            # "X is the answer"
            r'^(.+?)\s+is\s+(?:the\s+)?answer',
            # First sentence
            r'^(.+?)(?:\.|$)',
            # Just first few words
            r'^(\S+(?:\s+\S+){0,4})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                candidate = match.group(1).strip()
                
                # Validate based on answer type
                if self._validate_answer(candidate, answer_type, question):
                    return candidate
        
        # Fallback: return first word or short phrase
        words = response.split()
        if len(words) <= 5:
            return response
        return ' '.join(words[:3])
    
    def _detect_answer_type(self, question: str) -> str:
        """Detect expected answer type from question."""
        question_lower = question.lower()
        
        # Yes/No questions
        if any(question_lower.startswith(w) for w in ['is', 'are', 'was', 'were', 
                                                       'did', 'does', 'do', 'can', 
                                                       'could', 'would', 'should']):
            return 'yes_no'
        
        # Number questions
        if any(w in question_lower for w in ['how many', 'how much', 'number of']):
            return 'number'
        
        # Date questions
        if any(w in question_lower for w in ['when', 'what year', 'what date']):
            return 'date'
        
        # Person questions
        if question_lower.startswith('who'):
            return 'person'
        
        # Location questions
        if question_lower.startswith('where'):
            return 'location'
        
        return 'short_answer'
    
    def _validate_answer(self, answer: str, answer_type: str, question: str) -> bool:
        """Validate extracted answer based on type."""
        # Length check (answers shouldn't be too long)
        if len(answer.split()) > 15:
            return False
        
        if answer_type == 'yes_no':
            return answer.lower() in ['yes', 'no', 'true', 'false']
        
        elif answer_type == 'number':
            # Should contain digits
            return bool(re.search(r'\d', answer))
        
        elif answer_type == 'date':
            # Should contain year or date-like pattern
            return bool(re.search(r'\d{4}|\d{1,2}/\d{1,2}', answer))
        
        elif answer_type == 'person':
            # Should be capitalized (name)
            return answer[0].isupper() if answer else False
        
        elif answer_type == 'location':
            # Should be capitalized (place name)
            return answer[0].isupper() if answer else False
        
        # For short_answer, just check reasonable length
        return 1 <= len(answer.split()) <= 10


class ImprovedComplexReasoningEvaluator:
    """
    Enhanced evaluator that uses QueryDatasetOptimizer.
    """
    
    def __init__(self, tokenizer, device="cuda"):
        self.tokenizer = tokenizer
        self.device = device
        self.optimizer = QueryDatasetOptimizer(tokenizer)
    
    def evaluate_with_optimization(self, model, test_cases: List[Dict], 
                                   use_optimization: bool = True) -> Dict:
        """
        Evaluate with optional query optimization.
        """
        results = {
            'total': len(test_cases),
            'correct': 0,
            'details': []
        }
        
        logger.info(f"\n{'='*80}")
        logger.info(f"Evaluating {len(test_cases)} tasks...")
        logger.info(f"Optimization: {'ENABLED' if use_optimization else 'DISABLED'}")
        logger.info(f"{'='*80}")
        
        for i, case in enumerate(test_cases):
            # Get the prompt
            if 'context' in case and use_optimization:
                # This is a query dataset task - optimize it
                # Extract question from prompt if not directly available
                question = case.get('question', '')
                if not question and 'prompt' in case:
                    # Try to extract question from prompt
                    prompt_text = case['prompt']
                    if 'Question:' in prompt_text:
                        question = prompt_text.split('Question:')[-1].split('Answer:')[0].strip()
                    elif 'question' in case:
                        question = case['question']
                
                if question:
                    prompt = self.optimizer.optimize_query_task(
                        case['context'], 
                        question
                    )
                else:
                    # Fallback to original prompt
                    prompt = case.get('prompt', '')
            else:
                # Use existing prompt
                prompt = case.get('prompt', '')
                if not prompt and 'context' in case:
                    # Format standard way
                    question = case.get('question', '')
                    if question:
                        prompt = f"Context: {case['context']}\nQuestion: {question}\nAnswer:"
                    else:
                        prompt = f"Context: {case['context']}\nAnswer:"
            
            # Generate response
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            input_len = inputs['input_ids'].shape[1]
            response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
            
            # Extract answer (if optimization enabled)
            if use_optimization:
                # Get question for answer extraction
                question = case.get('question', '')
                if not question and 'prompt' in case:
                    prompt_text = case['prompt']
                    if 'Question:' in prompt_text:
                        question = prompt_text.split('Question:')[-1].split('Answer:')[0].strip()
                
                if question:
                    extracted = self.optimizer.extract_answer(response, question)
                else:
                    extracted = response.strip()
            else:
                extracted = response.strip()
            
            # Check correctness
            expected = case.get('expected', case.get('answer', ''))
            alternatives = case.get('alternatives', [expected])
            
            is_correct = self._smart_match(extracted, expected, alternatives)
            
            if is_correct:
                results['correct'] += 1
            
            # Store details
            results['details'].append({
                'task': case.get('task', case.get('question', '')[:50]),
                'expected': expected,
                'extracted': extracted[:80],
                'full_response': response[:100],
                'correct': is_correct
            })
            
            # Progress logging
            if (i + 1) % 10 == 0:
                current_acc = results['correct'] / (i + 1)
                logger.info(f"Progress: {i+1}/{len(test_cases)} - Accuracy: {current_acc*100:.1f}%")
        
        results['accuracy'] = results['correct'] / results['total']
        
        logger.info(f"\n{'='*80}")
        logger.info(f"Final Accuracy: {results['accuracy']*100:.1f}% ({results['correct']}/{results['total']})")
        logger.info(f"{'='*80}")
        
        return results
    
    def _smart_match(self, response: str, expected: str, alternatives: List[str]) -> bool:
        """Enhanced matching with normalization."""
        def normalize(text: str) -> str:
            # Lowercase, remove punctuation, extra spaces
            text = text.lower().strip()
            text = re.sub(r'[^\w\s]', '', text)
            text = re.sub(r'\s+', ' ', text)
            return text
        
        response_norm = normalize(response)
        expected_norm = normalize(expected)
        
        # Exact match
        if expected_norm in response_norm or response_norm in expected_norm:
            return True
        
        # Check alternatives
        for alt in alternatives:
            alt_norm = normalize(alt)
            if alt_norm in response_norm or response_norm in alt_norm:
                return True
        
        # Fuzzy match for short answers
        if len(expected_norm.split()) <= 3:
            # Check if main words are present
            expected_words = set(expected_norm.split())
            response_words = set(response_norm.split())
            overlap = expected_words & response_words
            if len(overlap) >= len(expected_words) * 0.7:  # 70% word overlap
                return True
        
        return False


def compare_optimization_impact(model, tokenizer, test_cases: List[Dict]):
    """
    Compare performance with and without optimization.
    
    This will show you exactly how much the optimization helps.
    
    Usage example:
        from mamba_model_loader import load_mamba_model_and_tokenizer
        from query_dataset_loader import get_query_tasks_for_evaluation
        from targeted_approach_6 import compare_optimization_impact
        
        # Load your model
        model, tokenizer = load_mamba_model_and_tokenizer(...)
        
        # Load query tasks
        query_tasks = get_query_tasks_for_evaluation(
            datasets=['squad', 'triviaqa'],
            num_per_dataset=40
        )
        
        # Flatten tasks
        all_tasks = []
        for task_list in query_tasks.values():
            all_tasks.extend(task_list)
        
        # Compare!
        results = compare_optimization_impact(model, tokenizer, all_tasks)
    """
    device = next(model.parameters()).device
    evaluator = ImprovedComplexReasoningEvaluator(tokenizer, device)
    
    logger.info("\n" + "="*80)
    logger.info("üî¨ OPTIMIZATION IMPACT COMPARISON")
    logger.info("="*80)
    
    # Test WITHOUT optimization
    logger.info("\nüìä Testing WITHOUT optimization (baseline)...")
    baseline_results = evaluator.evaluate_with_optimization(
        model, test_cases, use_optimization=False
    )
    
    # Test WITH optimization
    logger.info("\nüìä Testing WITH optimization...")
    optimized_results = evaluator.evaluate_with_optimization(
        model, test_cases, use_optimization=True
    )
    
    # Compare
    baseline_acc = baseline_results['accuracy']
    optimized_acc = optimized_results['accuracy']
    improvement = optimized_acc - baseline_acc
    
    logger.info("\n" + "="*80)
    logger.info("üìä RESULTS COMPARISON")
    logger.info("="*80)
    logger.info(f"Baseline (no optimization):  {baseline_acc*100:5.1f}%")
    logger.info(f"With optimization:           {optimized_acc*100:5.1f}%")
    logger.info(f"Improvement:                 {improvement*100:+5.1f}%")
    
    if improvement > 0.15:
        logger.info("\n‚úÖ EXCELLENT: Optimization provides major improvement!")
    elif improvement > 0.05:
        logger.info("\nüìà GOOD: Optimization provides meaningful improvement")
    elif improvement > 0:
        logger.info("\nüìä MODEST: Optimization provides slight improvement")
    else:
        logger.info("\n‚ö†Ô∏è WARNING: Optimization may not be helping")
    
    # Show specific examples where optimization helped
    logger.info("\n" + "="*80)
    logger.info("üîç EXAMPLE IMPROVEMENTS")
    logger.info("="*80)
    
    helped = []
    hurt = []
    
    for i, (base_detail, opt_detail) in enumerate(zip(
        baseline_results['details'][:20], 
        optimized_results['details'][:20]
    )):
        if not base_detail['correct'] and opt_detail['correct']:
            helped.append((i, base_detail, opt_detail))
        elif base_detail['correct'] and not opt_detail['correct']:
            hurt.append((i, base_detail, opt_detail))
    
    if helped:
        logger.info(f"\n‚úÖ Cases FIXED by optimization ({len(helped)} shown):")
        for i, base, opt in helped[:5]:
            logger.info(f"\nCase {i+1}:")
            logger.info(f"  Expected: {base['expected']}")
            logger.info(f"  Before: {base['extracted'][:60]}... ‚ùå")
            logger.info(f"  After:  {opt['extracted'][:60]}... ‚úÖ")
    
    if hurt:
        logger.info(f"\n‚ö†Ô∏è Cases BROKEN by optimization ({len(hurt)} shown):")
        for i, base, opt in hurt[:3]:
            logger.info(f"\nCase {i+1}:")
            logger.info(f"  Expected: {base['expected']}")
            logger.info(f"  Before: {base['extracted'][:60]}... ‚úÖ")
            logger.info(f"  After:  {opt['extracted'][:60]}... ‚ùå")
    
    return {
        'baseline': baseline_results,
        'optimized': optimized_results,
        'improvement': improvement,
        'helped': len(helped),
        'hurt': len(hurt)
    }


# Integration with your existing code
def integrate_with_existing_evaluation():
    """
    Example of how to integrate this optimizer with your existing code.
    """
    from mamba_model_loader import load_mamba_model_and_tokenizer
    from query_dataset_loader import get_query_tasks_for_evaluation
    
    logger.info("="*80)
    logger.info("üöÄ RUNNING OPTIMIZED EVALUATION")
    logger.info("="*80)
    
    # Load model
    model, tokenizer = load_mamba_model_and_tokenizer(
        model_name="state-spaces/mamba-130m-hf",
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_mamba_class=True,
        fallback_to_auto=True
    )
    
    device = next(model.parameters()).device
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load query dataset tasks
    logger.info("\nüìö Loading query dataset tasks...")
    query_tasks = get_query_tasks_for_evaluation(
        datasets=['squad', 'triviaqa'],
        num_per_dataset=40
    )
    
    # Flatten all tasks
    all_tasks = []
    for task_list in query_tasks.values():
        all_tasks.extend(task_list)
    
    logger.info(f"‚úÖ Loaded {len(all_tasks)} query tasks")
    
    # Run comparison
    comparison = compare_optimization_impact(model, tokenizer, all_tasks)
    
    logger.info("\n" + "="*80)
    logger.info("üí° RECOMMENDATIONS")
    logger.info("="*80)
    
    improvement = comparison['improvement']
    
    if improvement > 0.15:
        logger.info("‚úÖ Optimization is working VERY WELL!")
        logger.info("   ‚Üí Use optimized prompts for all query tasks")
        logger.info("   ‚Üí Expected final accuracy: 55-70%")
    elif improvement > 0.05:
        logger.info("üìà Optimization is helping")
        logger.info("   ‚Üí Use optimization for query tasks")
        logger.info("   ‚Üí Expected final accuracy: 45-60%")
    elif improvement > 0:
        logger.info("üìä Optimization provides slight benefit")
        logger.info("   ‚Üí Consider combining with other strategies")
    else:
        logger.info("‚ö†Ô∏è Optimization not helping significantly")
        logger.info("   ‚Üí May need fine-tuning or different approach")
        logger.info("   ‚Üí Check context compression settings")
    
    return comparison


# ============================================================================
# COMPLETE RUNNABLE FIX FOR QUERY TASKS
# ============================================================================

"""
COMPLETE RUNNABLE FIX FOR QUERY TASKS

======================================

Drop this into your targeted_approach_7.py or run standalone.

Expected improvement: 40% ‚Üí 62-68%

Usage:
    python targeted_approach_7.py --query_fix
"""


def run_complete_query_fix():
    """
    Complete test showing before/after on query tasks.
    """
    logger.info("="*80)
    logger.info("üöÄ QUERY TASK FIX - COMPLETE TEST")
    logger.info("="*80)
    
    # Step 1: Load model
    logger.info("\nüì¶ Step 1: Loading model...")
    from mamba_model_loader import load_mamba_model_and_tokenizer
    
    model, tokenizer = load_mamba_model_and_tokenizer(
        model_name="state-spaces/mamba-130m-hf",
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_mamba_class=True,
        fallback_to_auto=True
    )
    
    device = next(model.parameters()).device
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    logger.info(f"‚úÖ Model loaded on {device}")
    
    # Step 2: Load query tasks
    logger.info("\nüìö Step 2: Loading query tasks...")
    test_cases = load_query_tasks_for_fix()
    logger.info(f"‚úÖ Loaded {len(test_cases)} query tasks")
    
    # Step 3: Baseline (current approach - Cluster 9 steering)
    logger.info("\nüìä Step 3: Baseline with Cluster 9 steering...")
    baseline_results = run_baseline_cluster9(model, tokenizer, test_cases)
    baseline_acc = baseline_results['accuracy'] * 100
    logger.info(f"   Baseline accuracy: {baseline_acc:.1f}%")
    
    # Step 4: New approach (query-specific multi-layer steering)
    logger.info("\nüìä Step 4: New query-specific steering...")
    
    improved_results = improved_query_evaluation(
        model, tokenizer, test_cases,
        discover_neurons=True  # Will find query-specific neurons
    )
    improved_acc = improved_results['accuracy'] * 100
    logger.info(f"   Improved accuracy: {improved_acc:.1f}%")
    
    # Step 5: Analysis
    logger.info("\n" + "="*80)
    logger.info("üìä RESULTS COMPARISON")
    logger.info("="*80)
    
    improvement = improved_acc - baseline_acc
    
    logger.info(f"\n{'Method':<35} {'Accuracy':<12} {'Change'}")
    logger.info("-"*70)
    logger.info(f"{'Baseline (Cluster 9 @ Layer 20)':<35} {baseline_acc:>6.1f}%      {'‚Äî'}")
    logger.info(f"{'New (Query neurons @ 3 layers)':<35} {improved_acc:>6.1f}%      {improvement:>+6.1f}%")
    
    # Show example improvements
    show_example_improvements(baseline_results, improved_results, test_cases)
    
    # Save results
    save_comparison(baseline_results, improved_results)
    
    # Final verdict
    logger.info("\n" + "="*80)
    logger.info("üéØ FINAL VERDICT")
    logger.info("="*80)
    
    if improvement > 20:
        logger.info(f"‚úÖ EXCELLENT: +{improvement:.1f}% improvement!")
        logger.info(f"   Target achieved (40% ‚Üí {improved_acc:.1f}%)")
        logger.info(f"\n   Recommendations:")
        logger.info(f"   ‚úÖ Replace Cluster 9 steering with query-specific approach")
        logger.info(f"   ‚úÖ Use this for all query dataset evaluations")
        logger.info(f"   ‚úÖ Document in research paper")
    elif improvement > 10:
        logger.info(f"üìà GOOD: +{improvement:.1f}% improvement")
        logger.info(f"   Meaningful progress (40% ‚Üí {improved_acc:.1f}%)")
        logger.info(f"\n   Recommendations:")
        logger.info(f"   ‚úÖ Use query-specific steering for queries")
        logger.info(f"   üîß Try adjusting strength parameter")
        logger.info(f"   üìä Consider fine-tuning for further gains")
    elif improvement > 5:
        logger.info(f"üìä MODEST: +{improvement:.1f}% improvement")
        logger.info(f"   Some progress (40% ‚Üí {improved_acc:.1f}%)")
        logger.info(f"\n   Recommendations:")
        logger.info(f"   üîß Increase steering strength to 4.5-5.0")
        logger.info(f"   üî¨ Analyze neuron discovery results")
        logger.info(f"   üìö Try with more training examples")
    else:
        logger.info(f"‚ö†Ô∏è LIMITED: +{improvement:.1f}% improvement")
        logger.info(f"   Minimal progress (40% ‚Üí {improved_acc:.1f}%)")
        logger.info(f"\n   Possible issues:")
        logger.info(f"   üîç Neuron discovery may not have found optimal neurons")
        logger.info(f"   ü§ñ Model may be at capability limit")
        logger.info(f"   üìä Consider Mamba-370M or fine-tuning")
    
    return {
        'baseline': baseline_results,
        'improved': improved_results,
        'improvement': improvement
    }


def load_query_tasks_for_fix():
    """Load query tasks from datasets or create synthetic."""
    try:
        from query_dataset_loader import get_query_tasks_for_evaluation
        
        query_tasks = get_query_tasks_for_evaluation(
            datasets=['squad'],
            num_per_dataset=20
        )
        
        test_cases = []
        for task_list in query_tasks.values():
            test_cases.extend(task_list)
        
        return test_cases
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not load datasets: {e}")
        logger.info("   Using synthetic test cases...")
        
        # Return your actual failed cases from the log
        return [
            {
                'context': 'Almost all ctenophores are predators, taking prey ranging from microscopic larvae and rotifers to the adults of small crustaceans; the exceptions are juveniles of two species, which live as parasites on the salps on which adults of their species feed. In favorable circumstances, ctenophores can eat ten times their own weight in a day. Only 100‚Äì150 species have been validated, and possibly another 25 have not been fully described and named. The textbook examples are cydippids with egg-shaped bodies and a pair of retractable tentacles fringed with tentilla ("little tentacles") that are covered with colloblasts, sticky cells that capture prey.',
                'question': 'What are the little tentacles called?',
                'expected': 'tentilla',
                'alternatives': ['tentilla', 'Tentilla']
            },
            {
                'context': 'In November 1969, Yankee Clipper astronaut Charles "Pete" Conrad and LMP Alan L. Bean made a precision landing on Apollo 12 within walking distance (about 200 meters) of the Surveyor 3 probe, which landed in April 1967 on the Ocean of Storms.',
                'question': 'What was the name of the unmanned probe landed on the moon before Apollo 12?',
                'expected': 'Surveyor 3',
                'alternatives': ['Surveyor 3', 'surveyor 3']
            },
            {
                'context': 'On 18 April 1521, Luther appeared as ordered before the Diet of Worms. This was a general assembly of the estates of the Holy Roman Empire that took place in Worms, a town on the Rhine. It was conducted from 28 January to 25 May 1521, with Emperor Charles V presiding.',
                'question': 'When was the Edict of Worms presented?',
                'expected': '25 May 1521',
                'alternatives': ['25 May 1521', 'May 25, 1521']
            },
            # Add more from your log...
        ] * 7  # Repeat to get ~20 cases


def run_baseline_cluster9(model, tokenizer, test_cases):
    """
    Baseline: Current approach with Cluster 9 steering.
    """
    logger.info("   Using Cluster 9 neurons at Layer 20...")
    
    # Use SimpleSteering from this file
    steering = SimpleSteering(model)
    steering.apply_steering(strength=5.0, layer_idx=20)
    
    # Evaluate
    device = next(model.parameters()).device
    results = {'correct': 0, 'total': len(test_cases), 'details': []}
    
    for case in test_cases:
        context = case.get('context', '')
        question = case.get('question', '')
        expected = case.get('expected', '')
        
        # Use basic prompt
        prompt = f"Context: {context}\nQuestion: {question}\nAnswer:"
        
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        input_len = inputs['input_ids'].shape[1]
        response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)
        
        # Simple matching
        is_correct = expected.lower() in response.lower()
        
        if is_correct:
            results['correct'] += 1
        
        results['details'].append({
            'question': question[:60],
            'expected': expected,
            'response': response[:80],
            'correct': is_correct
        })
    
    steering.remove_steering()
    
    results['accuracy'] = results['correct'] / results['total']
    
    return results


def show_example_improvements(baseline_results, improved_results, test_cases):
    """Show specific examples where improvement helped."""
    logger.info("\n" + "="*80)
    logger.info("üîç EXAMPLE IMPROVEMENTS")
    logger.info("="*80)
    
    # Find cases that improved
    improved_cases = []
    
    for i, (base, imp) in enumerate(zip(baseline_results['details'], improved_results['details'])):
        if not base['correct'] and imp['correct']:
            improved_cases.append((i, base, imp, test_cases[i]))
    
    if improved_cases:
        logger.info(f"\n‚úÖ Cases FIXED by new approach (showing up to 5):")
        
        for i, base, imp, case in improved_cases[:5]:
            logger.info(f"\nExample {i+1}:")
            logger.info(f"  Question: {case['question'][:70]}...")
            logger.info(f"  Expected: '{case['expected']}'")
            logger.info(f"  Baseline: '{base['response']}...' ‚ùå")
            logger.info(f"  Improved: '{imp['response']}' ‚úÖ")
        
        logger.info(f"\nüí° Total cases fixed: {len(improved_cases)}/{len(test_cases)}")
    else:
        logger.info("\n‚ö†Ô∏è No cases were fixed (both approaches have same errors)")


def save_comparison(baseline_results, improved_results):
    """Save comparison results."""
    output_path = Path("experiment_logs/query_comparison.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    data = {
        'baseline': {
            'method': 'Cluster 9 neurons @ Layer 20',
            'accuracy': float(baseline_results['accuracy']),
            'correct': baseline_results['correct'],
            'total': baseline_results['total']
        },
        'improved': {
            'method': 'Query-specific neurons @ Layers 19,20,21',
            'accuracy': float(improved_results['accuracy']),
            'correct': improved_results['correct'],
            'total': improved_results['total']
        },
        'improvement': {
            'absolute': float(improved_results['accuracy'] - baseline_results['accuracy']) * 100,
            'relative': float((improved_results['accuracy'] / max(baseline_results['accuracy'], 0.01) - 1) * 100)
        }
    }
    
    with open(output_path, 'w') as f:
        json.dump(data, f, indent=2)
    
    logger.info(f"\nüíæ Results saved to: {output_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Capability assessment, diagnostics, or optimization")
    parser.add_argument("--trained_model", type=str, default=None,
                       help="Path to model trained on The Pile")
    parser.add_argument("--query_datasets", type=str, nargs='+', 
                       default=['squad'],
                       help="Query datasets to test on (squad, natural_questions, triviaqa)")
    parser.add_argument("--diagnostics", action="store_true",
                       help="Run comprehensive steering diagnostics instead of assessment")
    parser.add_argument("--optimize", action="store_true",
                       help="Run query dataset optimization comparison")
    parser.add_argument("--complete_test", action="store_true",
                       help="Run complete test with detailed analysis and recommendations")
    parser.add_argument("--query_fix", action="store_true",
                       help="Run complete query fix comparison (baseline vs improved)")
    parser.add_argument("--use_custom_prompts", action="store_true", default=True,
                       help="Use custom prompts from prompt_generator_100.py (default: True)")
    parser.add_argument("--no_custom_prompts", action="store_false", dest="use_custom_prompts",
                       help="Don't use custom prompts")
    parser.add_argument("--use_dataset_prompts", action="store_true",
                       help="Use classified dataset prompts from file")
    parser.add_argument("--dataset_file", type=str, default=None,
                       help="Path to classified dataset questions JSON file")
    parser.add_argument("--max_per_category", type=int, default=100,
                       help="Maximum questions per category from datasets (default: 100)")
    parser.add_argument("--create_dataset_file", action="store_true",
                       help="Create classified dataset file before running evaluation")
    parser.add_argument("--datasets_to_classify", type=str, nargs='+',
                       default=['squad', 'hotpotqa', 'triviaqa', 'musique', 'drop', 'natural_questions'],
                       help="Datasets to classify when creating file (squad for Simple Recall/Long Context, natural_questions for Stress Test)")
    parser.add_argument("--num_per_dataset", type=int, default=100,
                       help="Initial number of samples per dataset when creating file")
    parser.add_argument("--target_per_category", type=int, default=100,
                       help="Target number of questions per category from datasets (default: 100)")
    
    args = parser.parse_args()
    
    # Create dataset file if requested
    if args.create_dataset_file:
        logger.info("="*80)
        logger.info("üìä CREATING CLASSIFIED DATASET FILE")
        logger.info("="*80)
        try:
            from dataset_classifier import create_classified_dataset_file
            classified = create_classified_dataset_file(
                datasets=args.datasets_to_classify,
                num_per_dataset=args.num_per_dataset,
                target_per_category=args.target_per_category,
                output_file=args.dataset_file or "experiment_logs/classified_dataset_questions.json"
            )
            logger.info("‚úÖ Dataset classification complete!")
        except Exception as e:
            logger.error(f"‚ùå Error creating dataset file: {e}")
            import traceback
            logger.error(traceback.format_exc())
            import sys
            sys.exit(1)
    
    if args.complete_test:
        results = run_complete_test()
        
        # Save results
        import json
        from pathlib import Path
        
        output_path = Path("experiment_logs/query_optimization_results.json")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Prepare serializable results
        save_data = {
            'baseline_accuracy': float(results['baseline']['accuracy']),
            'optimized_accuracy': float(results['optimized']['accuracy']),
            'improvement_pct': float(results['improvement_pct']),
            'baseline_correct': results['baseline']['correct'],
            'baseline_total': results['baseline']['total'],
            'optimized_correct': results['optimized']['correct'],
            'optimized_total': results['optimized']['total'],
            'high_confidence_correct_baseline': results['baseline']['high_confidence_correct'],
            'high_confidence_correct_optimized': results['optimized']['high_confidence_correct'],
        }
        
        with open(output_path, 'w') as f:
            json.dump(save_data, f, indent=2)
        
        logger.info(f"\nüíæ Results saved to: {output_path}")
        logger.info(f"\n{'='*80}")
        logger.info(f"üéâ TEST COMPLETE")
        logger.info(f"{'='*80}")
    elif args.query_fix:
        results = run_complete_query_fix()
    elif args.optimize:
        results = integrate_with_existing_evaluation()
    elif args.diagnostics:
        results = run_comprehensive_diagnostics()
    else:
        results = run_capability_assessment(
            trained_model_path=args.trained_model,
            query_datasets=args.query_datasets,
            use_custom_prompts=args.use_custom_prompts,
            use_dataset_prompts=args.use_dataset_prompts,
            dataset_file=args.dataset_file,
            max_per_category=args.max_per_category
        )